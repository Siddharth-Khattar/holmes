---
phase: 03-file-ingestion
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - backend/app/api/files.py
  - backend/app/services/file_service.py
  - backend/app/main.py
  - backend/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Files can be uploaded via POST /api/cases/{case_id}/files"
    - "Files are streamed to GCS in chunks (no memory issues)"
    - "Content hash is computed during upload for duplicate detection"
    - "File metadata is stored in database"
    - "Invalid MIME types are rejected with 400 error"
  artifacts:
    - path: "backend/app/api/files.py"
      provides: "File upload endpoint"
      contains: "async def upload_file"
    - path: "backend/app/services/file_service.py"
      provides: "GCS upload logic with chunked streaming"
      contains: "async def upload_to_gcs"
  key_links:
    - from: "backend/app/api/files.py"
      to: "backend/app/services/file_service.py"
      via: "service import"
      pattern: "from app.services.file_service import"
    - from: "backend/app/services/file_service.py"
      to: "backend/app/storage.py"
      via: "get_bucket import"
      pattern: "from app.storage import get_bucket"
---

<objective>
Implement file upload API endpoint with GCS streaming.

Purpose: Enable users to upload evidence files to their cases. Files are streamed directly to GCS in chunks to handle 500MB files without memory issues, and metadata is persisted to the database.

Output: Working upload endpoint that accepts multipart file uploads, validates types, streams to GCS, computes content hash, and saves metadata.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-file-ingestion/03-CONTEXT.md
@.planning/phases/03-file-ingestion/03-RESEARCH.md
@.planning/phases/03-file-ingestion/03-01-SUMMARY.md
@backend/app/api/cases.py
@backend/app/storage.py
@backend/app/api/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add python-multipart dependency</name>
  <files>backend/pyproject.toml</files>
  <action>
Add `python-multipart` to the backend dependencies. This is required for FastAPI to parse multipart form data (file uploads).

```bash
cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend
source .venv/bin/activate
uv add python-multipart
```

Verify it's added to pyproject.toml dependencies.
  </action>
  <verify>
```bash
cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend
grep "python-multipart" pyproject.toml
```
  </verify>
  <done>python-multipart is in pyproject.toml dependencies</done>
</task>

<task type="auto">
  <name>Task 2: Create file service with GCS upload logic</name>
  <files>backend/app/services/file_service.py</files>
  <action>
Create `backend/app/services/file_service.py` with helper functions:

1. **ALLOWED_MIME_TYPES** constant - whitelist per CONTEXT.md:
   - Documents: application/pdf, application/vnd.openxmlformats-officedocument.wordprocessingml.document, .spreadsheetml.sheet, .presentationml.presentation
   - Images: image/jpeg, image/png, image/gif, image/webp
   - Video: video/mp4, video/quicktime, video/webm
   - Audio: audio/mpeg, audio/wav, audio/x-m4a

2. **MAX_FILE_SIZE** = 500 * 1024 * 1024 (500MB)

3. **CHUNK_SIZE** = 8 * 1024 * 1024 (8MB chunks)

4. **detect_category(mime_type: str) -> FileCategory**:
   - Map MIME type to FileCategory enum
   - Documents: application/* (pdf, docx, xlsx, pptx)
   - Images: image/*
   - Video: video/*
   - Audio: audio/*

5. **async def upload_to_gcs(file: UploadFile, case_id: UUID, file_uuid: UUID) -> tuple[str, int, str]**:
   - Construct storage_path: `cases/{case_id}/files/{file_uuid}.{ext}`
   - Get bucket from storage.py
   - Stream upload in CHUNK_SIZE chunks while computing SHA-256 hash
   - Track total_bytes, validate against MAX_FILE_SIZE during upload
   - If exceeds limit, delete partial blob and raise ValueError
   - Return (storage_path, total_bytes, content_hash)

6. **async def delete_from_gcs(storage_path: str) -> bool**:
   - Delete blob from GCS
   - Return True if deleted, False if not found

Add ABOUTME comment. Import FileCategory from app.models.file.
  </action>
  <verify>
```bash
cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend
source .venv/bin/activate
python -c "from app.services.file_service import upload_to_gcs, delete_from_gcs, detect_category, ALLOWED_MIME_TYPES; print('Service imports OK')"
```
  </verify>
  <done>File service exists with upload_to_gcs, delete_from_gcs, detect_category functions</done>
</task>

<task type="auto">
  <name>Task 3: Create files router with upload endpoint</name>
  <files>backend/app/api/files.py, backend/app/main.py</files>
  <action>
Create `backend/app/api/files.py` with upload endpoint:

1. **Router setup**:
   ```python
   router = APIRouter(prefix="/api/cases/{case_id}/files", tags=["files"])
   ```

2. **Helper function get_user_case(db, case_id, user_id)**:
   - Fetch case ensuring ownership and not deleted
   - Return Case or None

3. **POST endpoint** `@router.post("", status_code=201, response_model=FileResponse)`:
   - Path param: case_id: UUID
   - Form params: file: UploadFile, description: str | None = Form(None)
   - Dependencies: current_user: CurrentUser, db: AsyncSession

   Implementation:
   a. Validate case ownership using get_user_case
   b. Validate MIME type against ALLOWED_MIME_TYPES, return 400 if invalid
   c. Generate file_uuid = uuid4()
   d. Call upload_to_gcs(file, case_id, file_uuid) - wrap in try/except
   e. Check for duplicate: query CaseFile where case_id and content_hash match
   f. Detect category from mime_type
   g. Create CaseFile record with all fields
   h. Increment case.file_count
   i. Commit transaction
   j. Return FileResponse with duplicate_of set if duplicate found

   Error handling:
   - 404 if case not found
   - 400 if MIME type not allowed
   - 400 if file too large
   - 500 on GCS errors (log and return generic message)

4. **Register router** in `backend/app/main.py`:
   - Import: `from app.api.files import router as files_router`
   - Add: `app.include_router(files_router)`

Add ABOUTME comment. Follow patterns from cases.py for error responses, type hints, and structure.
  </action>
  <verify>
```bash
cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend
source .venv/bin/activate

# Check imports
python -c "from app.api.files import router; print('Router import OK')"

# Check main.py includes router
grep -n "files_router" app/main.py

# Type check
python -m py_compile app/api/files.py
```
  </verify>
  <done>Upload endpoint exists at POST /api/cases/{case_id}/files, router registered in main.py</done>
</task>

</tasks>

<verification>
```bash
cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend
source .venv/bin/activate

# Lint all new files
ruff check app/api/files.py app/services/file_service.py

# Type check
python -m py_compile app/api/files.py
python -m py_compile app/services/file_service.py

# Verify FastAPI startup
python -c "from app.main import app; print('App starts OK')"

# Check endpoint is registered
python -c "
from app.main import app
routes = [r.path for r in app.routes]
assert '/api/cases/{case_id}/files' in routes, 'Files route not found'
print('Files endpoint registered')
"
```
</verification>

<success_criteria>
- python-multipart dependency added
- File service handles GCS upload with chunked streaming
- Content hash computed during upload (SHA-256)
- Upload endpoint validates MIME types against whitelist
- Upload endpoint validates file size (500MB max)
- Duplicate detection warns but allows upload (returns duplicate_of field)
- Case file_count incremented on successful upload
- Router registered in main.py
- All files pass linting and type checking
</success_criteria>

<output>
After completion, create `.planning/phases/03-file-ingestion/03-02-SUMMARY.md`
</output>
