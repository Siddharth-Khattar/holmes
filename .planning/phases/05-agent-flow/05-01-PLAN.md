---
phase: 05-agent-flow
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/agents/base.py
  - backend/app/services/agent_events.py
  - backend/app/api/agents.py
  - backend/app/api/sse.py
  - backend/app/agents/parsing.py
autonomous: true

must_haves:
  truths:
    - "SSE agent-complete events include token usage, duration, model, and thinking traces in metadata"
    - "Thinking traces stream in real-time as agents think (THINKING_UPDATE events), not only after completion"
    - "On SSE reconnect, frontend receives full state snapshot of all agent statuses"
    - "Pipeline-complete event includes total token usage and total duration"
  artifacts:
    - path: "backend/app/agents/base.py"
      provides: "after_model_callback extracts thinking parts and fires THINKING_UPDATE with full text"
      contains: "part.thought"
    - path: "backend/app/services/agent_events.py"
      provides: "New event types: THINKING_UPDATE streaming, CONFIRMATION_REQUIRED, STATE_SNAPSHOT, enriched PIPELINE_COMPLETE"
      contains: "emit_thinking_update"
    - path: "backend/app/api/sse.py"
      provides: "State snapshot sent on SSE connect, built from agent_executions table"
      contains: "build_state_snapshot"
    - path: "backend/app/api/agents.py"
      provides: "Enriched agent-complete payloads with metadata (tokens, duration, model, thinkingTraces)"
      contains: "inputTokens"
  key_links:
    - from: "backend/app/agents/base.py"
      to: "backend/app/services/agent_events.py"
      via: "publish_fn calls publish_agent_event for THINKING_UPDATE"
      pattern: "THINKING_UPDATE"
    - from: "backend/app/api/agents.py"
      to: "backend/app/services/agent_events.py"
      via: "emit_agent_complete with enriched metadata"
      pattern: "metadata.*inputTokens"
    - from: "backend/app/api/sse.py"
      to: "backend/app/api/agents.py"
      via: "build_state_snapshot queries agent_executions"
      pattern: "build_state_snapshot"
---

<objective>
Enrich backend SSE events with real-time thinking traces, token usage, timing data, and state snapshots for reconnection.

Purpose: The frontend Command Center UI is fully built (Phase 4.1) but receives only minimal SSE event data. This plan enriches all backend SSE events to provide the data the frontend needs: real-time thinking traces, per-agent token usage and duration, model identification, and state snapshots for reconnection.

Output: Backend SSE events carry full metadata; `after_model_callback` streams thinking traces in real-time; SSE reconnection sends state snapshot.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-agent-flow/05-CONTEXT.md
@.planning/phases/05-agent-flow/05-RESEARCH.md

@backend/app/agents/base.py
@backend/app/services/agent_events.py
@backend/app/api/agents.py
@backend/app/api/sse.py
@backend/app/agents/parsing.py
@backend/app/models/agent_execution.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enrich agent callbacks and add real-time thinking trace streaming</name>
  <files>
    backend/app/agents/base.py
    backend/app/services/agent_events.py
    backend/app/agents/parsing.py
  </files>
  <action>
  **In `backend/app/agents/base.py`:**

  1. Enhance the `after_model` callback inside `create_agent_callbacks()` to extract thinking parts from `llm_response.content.parts` where `part.thought == True` and `part.text` exists. Join all thinking parts into a single string and fire a `THINKING_UPDATE` event with the FULL text (no truncation -- CONTEXT.md says "Show full unfiltered thinking output"). The payload must include:
     - `case_id`: from closure
     - `agent_name`: from `callback_context.agent_name`
     - `timestamp`: ISO string
     - `thought`: full text of thinking parts joined with newlines

  2. Also extract token usage from `llm_response` if available. The `LlmResponse` object may have `usage_metadata` with `prompt_token_count`, `candidates_token_count`, `thoughts_token_count`. If present, fire a separate data point or include in the THINKING_UPDATE payload under a `tokenDelta` key:
     - `inputTokens`: prompt_token_count
     - `outputTokens`: candidates_token_count
     - `thoughtsTokens`: thoughts_token_count

  Important: The `after_model_callback` fires on EACH model turn. Thinking parts appear in `llm_response.content.parts` where `part.thought == True`. This is the same pattern used by `extract_thinking_traces()` in `parsing.py`.

  **In `backend/app/services/agent_events.py`:**

  1. Add new event types to `AgentEventType` enum:
     - `STATE_SNAPSHOT = "state-snapshot"`
     - `CONFIRMATION_REQUIRED = "confirmation-required"`
     - `CONFIRMATION_RESOLVED = "confirmation-resolved"`

  2. Add a new convenience emitter `emit_thinking_update()`:
     ```python
     async def emit_thinking_update(
         case_id: str,
         agent_type: str,
         thought: str,
         token_delta: dict[str, int] | None = None,
     ) -> None:
     ```
     This should publish a `THINKING_UPDATE` event with the thought text and optional token delta.

  3. Add `emit_confirmation_required()` and `emit_confirmation_resolved()` convenience emitters for the HITL system (will be used in Plan 02).

  **In `backend/app/agents/parsing.py`:**

  No changes needed -- the existing `extract_thinking_traces()` function caps at 2000 chars for DATABASE storage, which is correct. The SSE stream sends full text (no cap). Leave this file as-is.

  **Wiring `publish_fn`:** The `create_agent_callbacks()` function already accepts a `publish_fn` parameter and the `_fire` helper dispatches events. The key gap is that `run_triage()` and `run_orchestrator()` in `agents.py` (Plan 01 Task 2) need to pass a publish_fn that calls `publish_agent_event()`. However, the current callback approach in `_fire` uses synchronous `asyncio.ensure_future()` which works. The mapping from internal callback event names (AGENT_SPAWNED, THINKING_UPDATE, etc.) to `AgentEventType` enum values needs to happen in the publish_fn. Add a helper function `create_sse_publish_fn(case_id: str)` in `base.py` that creates a bound publish function mapping callback event types to `AgentEventType` values and calling `publish_agent_event()`.
  </action>
  <verify>
  - `python -c "from app.agents.base import create_agent_callbacks, create_sse_publish_fn"` succeeds
  - `python -c "from app.services.agent_events import emit_thinking_update, emit_confirmation_required, AgentEventType; print(AgentEventType.STATE_SNAPSHOT.value)"` prints "state-snapshot"
  - Run type checks: `cd backend && python -m mypy app/agents/base.py app/services/agent_events.py --ignore-missing-imports` (or pyright if configured)
  </verify>
  <done>
  - `after_model_callback` extracts thinking parts from `part.thought == True` and fires THINKING_UPDATE with full untruncated text
  - Token delta included in THINKING_UPDATE when `usage_metadata` is present on the response
  - New AgentEventType values: STATE_SNAPSHOT, CONFIRMATION_REQUIRED, CONFIRMATION_RESOLVED
  - New convenience emitters: `emit_thinking_update()`, `emit_confirmation_required()`, `emit_confirmation_resolved()`
  - `create_sse_publish_fn()` helper maps callback event names to AgentEventType and calls publish_agent_event
  </done>
</task>

<task type="auto">
  <name>Task 2: Enrich pipeline SSE events and add state snapshot on reconnect</name>
  <files>
    backend/app/api/agents.py
    backend/app/api/sse.py
  </files>
  <action>
  **In `backend/app/api/agents.py` (`run_analysis_workflow`):**

  1. Create a publish_fn using the new `create_sse_publish_fn(case_id)` helper from base.py. Pass this to `create_agent_callbacks()` and then pass those callbacks when creating agents in `run_triage()` and `run_orchestrator()`. This requires adding a `callbacks` parameter to `run_triage()` and `run_orchestrator()` -- check the function signatures and update them to accept and pass callbacks to the ADK agent. If they already accept callbacks, wire them. If not, add the parameter.

  2. Enrich the `emit_agent_complete()` call for TRIAGE with metadata:
     - Query the `AgentExecution` record for the triage agent (it's already queried later -- move or duplicate the query)
     - Include in the result dict under `"metadata"`:
       - `"inputTokens"`: from `execution.input_tokens`
       - `"outputTokens"`: from `execution.output_tokens`
       - `"durationMs"`: computed from `execution.started_at` and `execution.completed_at`
       - `"startedAt"`: ISO string from `execution.started_at`
       - `"completedAt"`: ISO string from `execution.completed_at`
       - `"model"`: `settings.gemini_flash_model` (for triage) or `settings.gemini_pro_model` (for orchestrator)
       - `"thinkingTraces"`: joined string from `execution.thinking_traces` list (each trace has a "thought" key)

  3. Do the same enrichment for the ORCHESTRATOR `emit_agent_complete()` call.

  4. Enrich `emit_processing_complete()` to include:
     - `"totalDurationMs"`: from `pipeline_start` to completion
     - `"totalInputTokens"`: sum of all execution input_tokens
     - `"totalOutputTokens"`: sum of all execution output_tokens

  5. Add `settings = get_settings()` import at top if not already there, for model name access.

  **In `backend/app/api/sse.py` (`command_center_generator`):**

  1. Add a `build_state_snapshot(case_id: str)` async function that:
     - Queries all `AgentExecution` records for the most recent workflow of this case
     - Builds a snapshot dict with each agent's current status (from execution status), last result metadata (tokens, duration, thinking traces), and timestamps
     - Returns a dict like: `{"agents": {"triage": {"status": "completed", "metadata": {...}}, "orchestrator": {"status": "running", ...}}}`

  2. Modify `command_center_generator()` to send a `state-snapshot` event immediately after subscribing to events and before entering the event loop. This requires importing db session factory. Use the same `_get_sessionmaker` pattern used in `agents.py` for background tasks.

  3. The snapshot event format: `{"event": "state-snapshot", "data": json.dumps(snapshot_dict)}`

  **Important:** Import `AgentExecution` and `AgentExecutionStatus` from `app.models.agent_execution`. Import `_get_sessionmaker` from `app.database`. Import `get_settings` from `app.config`.
  </action>
  <verify>
  - `cd backend && python -m py_compile app/api/agents.py && python -m py_compile app/api/sse.py` succeeds
  - Grep for enriched metadata: `grep -n "inputTokens" backend/app/api/agents.py` shows matches in both triage and orchestrator emit blocks
  - Grep for state snapshot: `grep -n "build_state_snapshot" backend/app/api/sse.py` shows the function definition and usage
  - Grep for total tokens: `grep -n "totalInputTokens\|totalOutputTokens\|totalDurationMs" backend/app/api/agents.py` shows enriched processing-complete event
  </verify>
  <done>
  - Triage and orchestrator agent-complete events include metadata: inputTokens, outputTokens, durationMs, startedAt, completedAt, model, thinkingTraces
  - Pipeline-complete event includes totalDurationMs, totalInputTokens, totalOutputTokens
  - publish_fn is wired from `create_sse_publish_fn()` through to agent callbacks so THINKING_UPDATE events fire during agent execution
  - SSE reconnect sends state-snapshot event with all agents' current status and metadata
  </done>
</task>

</tasks>

<verification>
1. Start backend server, trigger analysis, observe SSE events in browser devtools or curl:
   - `curl -N "http://localhost:8080/sse/cases/{case_id}/command-center/stream"` should immediately receive a `state-snapshot` event
   - Heartbeat events should appear every 15s
   - When analysis runs: `agent-started`, `thinking-update` (multiple, during processing), `agent-complete` (with metadata), `processing-complete` (with totals) events should appear
2. Agent-complete events contain metadata with inputTokens, outputTokens, durationMs, model, thinkingTraces
3. Processing-complete events contain totalDurationMs, totalInputTokens, totalOutputTokens
4. `python -m py_compile` succeeds for all modified files
</verification>

<success_criteria>
- All SSE agent lifecycle events carry enriched metadata (tokens, duration, model, thinking traces)
- Real-time THINKING_UPDATE events stream during agent processing (not after completion)
- State snapshot sent on SSE connection for reconnect resilience
- Pipeline-complete event includes aggregate statistics
- No regressions in existing SSE functionality (heartbeat, file events)
</success_criteria>

<output>
After completion, create `.planning/phases/05-agent-flow/05-01-SUMMARY.md`
</output>
