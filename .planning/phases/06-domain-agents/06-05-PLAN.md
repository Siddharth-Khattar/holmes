---
phase: 06-domain-agents
plan: 05
type: execute
wave: 3
depends_on: ["06-03", "06-04"]
files_modified:
  - backend/app/api/agents.py
  - backend/app/services/agent_events.py
autonomous: true

must_haves:
  truths:
    - "Full pipeline runs: Triage -> Orchestrator -> Parallel Domain Agents -> Strategy -> processing-complete"
    - "Each domain agent emits agent-started and agent-complete SSE events with metadata"
    - "Low-confidence findings trigger HITL confirmation via the existing confirmation service"
    - "Pipeline status endpoint reflects domain_analysis stage"
    - "Partial domain agent failures don't crash the pipeline"
    - "File statuses transition correctly through the extended pipeline"
    - "Existing triage-only and triage+orchestrator flows still work when orchestrator_output is None"
    - "Fallback usage visible in SSE events"
  artifacts:
    - path: "backend/app/api/agents.py"
      provides: "Extended run_analysis_workflow with domain agent stages 3, 4, and 5"
      contains: "run_domain_agents_parallel"
    - path: "backend/app/services/agent_events.py"
      provides: "emit_agent_fallback helper for fallback warning events"
      contains: "emit_agent_fallback"
  key_links:
    - from: "backend/app/api/agents.py"
      to: "backend/app/agents/domain_runner.py"
      via: "import and call run_domain_agents_parallel"
      pattern: "from app.agents.domain_runner import run_domain_agents_parallel"
    - from: "backend/app/api/agents.py"
      to: "backend/app/agents/strategy.py"
      via: "import and call run_strategy"
      pattern: "from app.agents.strategy import run_strategy"
    - from: "backend/app/api/agents.py"
      to: "backend/app/services/confirmation.py"
      via: "import request_confirmation for HITL"
      pattern: "from app.services.confirmation import request_confirmation"
    - from: "backend/app/api/agents.py"
      to: "backend/app/database.py"
      via: "session_factory = _get_sessionmaker() (already imported and used on line 179)"
      pattern: "session_factory = _get_sessionmaker()"
---

<objective>
Wire the domain agents into the analysis pipeline, add SSE event emission for domain agent lifecycle, integrate HITL confirmation for low-confidence findings, and update the pipeline status endpoint. This completes the Phase 6 backend implementation.

Purpose: This is the integration plan that connects all the pieces built in Plans 01-04 into the working pipeline. After this plan, uploading files and starting analysis will trigger the full pipeline through domain agents.

Output: Extended `agents.py` pipeline with stages 3-5 (domain + strategy + HITL), SSE events for all domain agents, updated status endpoint.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-domain-agents/06-CONTEXT.md
@.planning/phases/06-domain-agents/06-RESEARCH.md

@backend/app/api/agents.py
@backend/app/services/agent_events.py
@backend/app/services/confirmation.py
@backend/app/agents/domain_runner.py
@backend/app/agents/strategy.py
@backend/app/agents/base.py
@backend/app/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add emit_agent_fallback helper and wire Stage 3 (parallel domain agents)</name>
  <files>
    backend/app/services/agent_events.py
    backend/app/api/agents.py
  </files>
  <action>
**1a. Add `emit_agent_fallback` to `agent_events.py`:**

Add a new convenience emitter for fallback warnings:

```python
async def emit_agent_fallback(
    case_id: str,
    agent_type: str,
    fallback_model: str,
    reason: str | None = None,
) -> None:
    """Emit a thinking-update event indicating an agent fell back to a simpler model.

    Rendered as a warning badge on the agent node in the Command Center.
    Uses thinking-update event type since the frontend already handles it.

    Args:
        case_id: UUID string of the case.
        agent_type: Agent type that fell back (e.g., "financial").
        fallback_model: Name of the fallback model used.
        reason: Optional reason for the fallback.
    """
    warning_text = f"[FALLBACK] Agent switched to {fallback_model}"
    if reason:
        warning_text += f": {reason}"

    await publish_agent_event(
        case_id,
        AgentEventType.THINKING_UPDATE,
        {
            "type": AgentEventType.THINKING_UPDATE.value,
            "agentType": agent_type,
            "thought": warning_text,
            "timestamp": datetime.now(tz=UTC).isoformat(),
            "isFallback": True,
            "fallbackModel": fallback_model,
        },
    )
```

Add the `datetime` import at the top if not already present: `from datetime import UTC, datetime`

**1b. Add new imports to `agents.py`:**

Add these imports inside `run_analysis_workflow` (alongside the existing lazy imports for triage/orchestrator on lines 173-176) to avoid circular dependency:

```python
from app.agents.domain_runner import run_domain_agents_parallel, build_strategy_context
from app.agents.strategy import run_strategy
from app.agents.base import CONFIDENCE_THRESHOLD
from app.services.confirmation import request_confirmation
```

Note: `_get_sessionmaker` is ALREADY imported on line 176 and `session_factory` is ALREADY created on line 179. The domain_runner's `db_session_factory` parameter will receive this same `session_factory` variable. The type is `async_sessionmaker[AsyncSession]` which is a callable that returns `AsyncSession` context managers -- compatible with domain_runner's `Callable[..., AsyncContextManager[AsyncSession]]` type annotation.

**1c. Insert Stage 3 (Parallel Domain Agents) into `run_analysis_workflow`:**

After the orchestrator stage (after `emit_agent_complete` for orchestrator, around the current line 394 where "Step 6: Update file statuses to ANALYZED" is), insert Stage 3. The file status update and processing-complete will be moved in Task 2.

```python
# ---- Stage 3: Domain Agents (Parallel) ----
domain_results: dict[str, BaseModel | None] = {}

if orchestrator_output:
    logger.info(
        "Pipeline starting stage=domain_agents case=%s workflow=%s",
        case_id, workflow_id,
    )

    # Query orchestrator execution for parent chain
    orch_exec_result = await db.execute(
        select(AgentExecution)
        .where(
            AgentExecution.workflow_id == UUID(workflow_id),
            AgentExecution.agent_name == "orchestrator",
        )
        .order_by(AgentExecution.created_at.desc())
        .limit(1)
    )
    orch_execution = orch_exec_result.scalar_one_or_none()

    # Emit agent-started for each domain agent that will run
    domain_agents_to_run = set()
    for rd in orchestrator_output.routing_decisions:
        for agent_name in rd.target_agents:
            if agent_name in ("financial", "legal", "evidence"):
                domain_agents_to_run.add(agent_name)

    domain_task_ids: dict[str, str] = {}
    for agent_name in domain_agents_to_run:
        task_id = str(uuid4())
        domain_task_ids[agent_name] = task_id
        await emit_agent_started(
            case_id=case_id,
            agent_type=agent_name,
            task_id=task_id,
            file_id=str(first_file.id),
            file_name=f"{agent_name}-analysis",
        )

    # Run parallel domain agents (each creates own DB session via session_factory)
    domain_results = await run_domain_agents_parallel(
        case_id=case_id,
        workflow_id=workflow_id,
        user_id=user_id,
        routing=orchestrator_output,
        files=files,
        hypotheses=[],  # Empty until hypothesis system exists (Phase 7)
        db_session_factory=session_factory,
        publish_event=publish_fn,
        orchestrator_execution_id=orch_execution.id if orch_execution else None,
    )

    # Emit agent-complete/error for each domain agent
    for agent_name in domain_agents_to_run:
        task_id = domain_task_ids[agent_name]
        result = domain_results.get(agent_name)

        if result is not None:
            # Query execution record for metadata
            exec_result = await db.execute(
                select(AgentExecution)
                .where(
                    AgentExecution.workflow_id == UUID(workflow_id),
                    AgentExecution.agent_name == agent_name,
                )
                .order_by(AgentExecution.created_at.desc())
                .limit(1)
            )
            agent_exec = exec_result.scalar_one_or_none()
            agent_metadata = (
                _build_execution_metadata(agent_exec, settings.gemini_pro_model)
                if agent_exec else {}
            )

            finding_count = len(result.findings) if hasattr(result, 'findings') else 0
            entity_count = len(result.entities) if hasattr(result, 'entities') else 0

            await emit_agent_complete(
                case_id=case_id,
                agent_type=agent_name,
                task_id=task_id,
                result={
                    "taskId": task_id,
                    "agentType": agent_name,
                    "outputs": [
                        {
                            "type": f"{agent_name}-findings",
                            "data": {
                                "findingCount": finding_count,
                                "entityCount": entity_count,
                            },
                        }
                    ],
                    "metadata": agent_metadata,
                },
            )
        else:
            await emit_agent_error(
                case_id=case_id,
                agent_type=agent_name,
                task_id=task_id,
                error=f"{agent_name} agent failed to produce output",
            )
```
  </action>
  <verify>
1. `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && python -c "from app.services.agent_events import emit_agent_fallback; print('Fallback emitter OK')"` -- new helper importable.
2. `python -m py_compile app/api/agents.py && echo "Compilation OK"` -- no syntax errors after inserting Stage 3.
3. Verify by code inspection: When `orchestrator_output is None` (triage failure or orchestrator failure), `domain_results` stays as `{}` and Stage 3 is skipped entirely -- existing flow preserved.
  </verify>
  <done>
- emit_agent_fallback helper added to agent_events.py
- Stage 3 (parallel domain agents) wired into pipeline after orchestrator
- session_factory (already `_get_sessionmaker()` on line 179) passed to run_domain_agents_parallel -- type-compatible because async_sessionmaker is callable returning AsyncSession context managers
- Existing triage/orchestrator flow unchanged when orchestrator_output is None
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire Stage 4 (strategy) and Stage 5 (HITL), relocate file status update</name>
  <files>
    backend/app/api/agents.py
  </files>
  <action>
**2a. Insert Stage 4 (Strategy Agent) after Stage 3:**

```python
# ---- Stage 4: Legal Strategy Agent (Sequential, after domain agents) ----
strategy_result = None
any_domain_ran = any(v is not None for v in domain_results.values())

if orchestrator_output and any_domain_ran:
    logger.info(
        "Pipeline starting stage=strategy case=%s workflow=%s",
        case_id, workflow_id,
    )

    # Build text summaries of domain agent findings
    domain_summaries = build_strategy_context(domain_results)

    # Find strategy-routed files
    strategy_file_ids = set()
    for rd in orchestrator_output.routing_decisions:
        if "strategy" in rd.target_agents:
            strategy_file_ids.add(rd.file_id)

    file_lookup = {str(f.id): f for f in files}
    strategy_files = [file_lookup[fid] for fid in strategy_file_ids if fid in file_lookup]

    strategy_task_id = str(uuid4())
    await emit_agent_started(
        case_id=case_id,
        agent_type="strategy",
        task_id=strategy_task_id,
        file_id=str(first_file.id),
        file_name="strategy-analysis",
    )

    strategy_result = await run_strategy(
        case_id=case_id,
        workflow_id=workflow_id,
        user_id=user_id,
        files=strategy_files,
        domain_summaries=domain_summaries,
        hypotheses=[],
        db_session=db,
        publish_event=publish_fn,
        parent_execution_id=orch_execution.id if orch_execution else None,
    )

    if strategy_result:
        # Query strategy execution for metadata
        strat_exec_result = await db.execute(
            select(AgentExecution)
            .where(
                AgentExecution.workflow_id == UUID(workflow_id),
                AgentExecution.agent_name == "strategy",
            )
            .order_by(AgentExecution.created_at.desc())
            .limit(1)
        )
        strat_exec = strat_exec_result.scalar_one_or_none()
        strat_metadata = (
            _build_execution_metadata(strat_exec, settings.gemini_pro_model)
            if strat_exec else {}
        )

        await emit_agent_complete(
            case_id=case_id,
            agent_type="strategy",
            task_id=strategy_task_id,
            result={
                "taskId": strategy_task_id,
                "agentType": "strategy",
                "outputs": [
                    {
                        "type": "strategy-findings",
                        "data": {
                            "findingCount": len(strategy_result.findings),
                        },
                    }
                ],
                "metadata": strat_metadata,
            },
        )
        domain_results["strategy"] = strategy_result
    else:
        await emit_agent_error(
            case_id=case_id,
            agent_type="strategy",
            task_id=strategy_task_id,
            error="Strategy agent failed to produce output",
        )
```

**2b. Insert Stage 5 (HITL for low-confidence findings) after Stage 4:**

```python
# ---- Stage 5: HITL for Low-Confidence Findings ----
if domain_results:
    for agent_name, result in domain_results.items():
        if result is None or not hasattr(result, 'findings'):
            continue
        for finding in result.findings:
            if finding.confidence < CONFIDENCE_THRESHOLD:
                logger.info(
                    "Low-confidence finding from %s: %s (confidence=%s), requesting HITL",
                    agent_name, finding.title, finding.confidence,
                )
                confirmation_result = await request_confirmation(
                    case_id=case_id,
                    agent_type=agent_name,
                    action_description=(
                        f"Low-confidence finding ({finding.confidence}/100): {finding.title}"
                    ),
                    affected_items=[c.file_id for c in finding.citations],
                    context={
                        "finding_title": finding.title,
                        "finding_category": finding.category,
                        "finding_description": finding.description[:500],
                        "confidence": finding.confidence,
                        "agent": agent_name,
                    },
                )
                if not confirmation_result.approved:
                    logger.info(
                        "Finding rejected by user: %s from %s (reason: %s)",
                        finding.title, agent_name, confirmation_result.reason,
                    )
                    # Mark finding as rejected (for audit trail)
                    # Finding remains in output but will be excluded from KG in Phase 7
```

**2c. Relocate file status update and processing-complete:**

Move the existing "Step 6: Update file statuses to ANALYZED" and "Step 7: Emit processing-complete" to AFTER Stage 5 (HITL). This is a CUT+PASTE operation -- remove lines ~396-437 (the current Step 6 and Step 7 blocks) and paste them after Stage 5 with the following enhancement:

```python
# ---- Final: Update file statuses to ANALYZED ----
await db.execute(
    update(CaseFile)
    .where(CaseFile.id.in_([UUID(fid) for fid in file_ids]))
    .values(status=FileStatus.ANALYZED)
)
await db.commit()

# ---- Final: Emit processing-complete ----
# Count findings and entities across all domain agents
total_findings = 0
total_domain_entities = 0
for result in domain_results.values():
    if result is not None and hasattr(result, 'findings'):
        total_findings += len(result.findings)
    if result is not None and hasattr(result, 'entities'):
        total_domain_entities += len(result.entities)

total_entities = sum(len(fr.entities) for fr in triage_output.file_results)
total_duration_s = time.monotonic() - pipeline_start
total_duration_ms = int(total_duration_s * 1000)

# Aggregate token usage across all executions in this workflow
all_exec_result = await db.execute(
    select(AgentExecution).where(
        AgentExecution.workflow_id == UUID(workflow_id),
    )
)
all_executions = list(all_exec_result.scalars().all())
total_input_tokens = sum(e.input_tokens or 0 for e in all_executions)
total_output_tokens = sum(e.output_tokens or 0 for e in all_executions)

await emit_processing_complete(
    case_id=case_id,
    files_processed=len(files),
    entities_created=total_entities + total_domain_entities,
    relationships_created=0,  # Relationships created by KG Agent in Phase 7
    total_duration_ms=total_duration_ms,
    total_input_tokens=total_input_tokens,
    total_output_tokens=total_output_tokens,
)
```

**CRITICAL: Preserve triage-only flow.**
When `orchestrator_output is None`, `domain_results` will be `{}`. Stage 3 is skipped (guarded by `if orchestrator_output`). Stage 4 is skipped (guarded by `if orchestrator_output and any_domain_ran`). Stage 5 is skipped (guarded by `if domain_results`). The file status update and processing-complete STILL run -- files get marked ANALYZED and the pipeline completes normally. This preserves backward compatibility with the existing triage+orchestrator-only flow.

**IMPORTANT:** Do NOT move or modify the existing triage and orchestrator stages (lines ~217-394). Only ADD code after the orchestrator stage and adjust where the file status update / processing-complete happen.
  </action>
  <verify>
1. `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && python -m py_compile app/api/agents.py && echo "Compilation OK"` -- no syntax errors.
2. Code review: confirm that when `orchestrator_output is None`, the pipeline skips Stages 3-5 and still updates file statuses to ANALYZED and emits processing-complete (same behavior as before this change).
3. Code review: confirm that when `orchestrator_output` exists but all domain agents fail (domain_results has only None values), `any_domain_ran` is False, Strategy is skipped, but files still get marked ANALYZED.
  </verify>
  <done>
- Stage 4 (Strategy agent, sequential after parallel agents) wired in
- Stage 5 (HITL confirmation for low-confidence findings) wired in
- File status update and processing-complete moved to after all stages
- Existing triage-only and triage+orchestrator flows preserved (all new stages guarded by conditionals)
- Entity counts from domain agents included in processing-complete event
  </done>
</task>

<task type="auto">
  <name>Task 3: Update status endpoint and response schema for domain agents</name>
  <files>
    backend/app/api/agents.py
  </files>
  <action>
**3a. Update `AnalysisStatusResponse` to include domain results:**

Add an optional field to the response schema:
```python
domain_results_summary: dict[str, int] | None = Field(
    default=None,
    description="Summary of domain agent findings: agent_name -> finding_count",
)
```

**3b. Update `get_analysis_status` endpoint pipeline status derivation:**

Currently the status logic (lines ~599-631) only checks triage and orchestrator executions. Extend it to include domain agent stages:

In the loop over executions (around line 602), also collect domain agent executions:
```python
domain_execs: dict[str, AgentExecution] = {}
strategy_exec = None
for exec_record in executions:
    if exec_record.agent_name == "triage":
        triage_exec = exec_record
    elif exec_record.agent_name == "orchestrator":
        orchestrator_exec = exec_record
    elif exec_record.agent_name in ("financial", "legal", "evidence"):
        domain_execs[exec_record.agent_name] = exec_record
    elif exec_record.agent_name == "strategy":
        strategy_exec = exec_record
```

Update pipeline_status derivation to check domain agents:
- If strategy_exec completed -> status = "complete", completed_at = strategy_exec.completed_at
- If any domain agent or strategy is RUNNING -> status = "domain_analysis"
- If all domain agents COMPLETED but strategy not started -> status = "domain_analysis"
- If orchestrator completed but no domain agents exist -> status = "complete" (backward compat)
- Keep existing triage/orchestrating/pending/error logic as fallbacks

Build domain_results_summary from domain execution records:
```python
domain_summary: dict[str, int] | None = None
if domain_execs:
    domain_summary = {}
    for agent_name, exec_record in domain_execs.items():
        if exec_record.output_data and isinstance(exec_record.output_data, dict):
            findings = exec_record.output_data.get("findings", [])
            domain_summary[agent_name] = len(findings) if isinstance(findings, list) else 0
        else:
            domain_summary[agent_name] = 0
```

Include `domain_results_summary=domain_summary` in the response.

**3c. Update the docstring of `run_analysis_workflow`** to reflect the full pipeline:

Change the Steps docstring to:
```
Steps:
1. Update file statuses to QUEUED
2. Stage 1: Run Triage Agent (fresh session, multimodal files)
3. Stage 2: Run Orchestrator Agent (fresh session, text-only input)
4. Stage 3: Run Domain Agents (parallel fresh sessions)
5. Stage 4: Run Strategy Agent (sequential, receives domain summaries)
6. Stage 5: HITL for low-confidence findings
7. Update file statuses to ANALYZED
8. Emit processing-complete event
```

Also update the `start_analysis` endpoint docstring to remove "(Future)" from domain agents and synthesis references.
  </action>
  <verify>
1. `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && python -m py_compile app/api/agents.py && echo "Compilation OK"` -- no syntax errors.
2. `python -c "from app.api.agents import AnalysisStatusResponse; r = AnalysisStatusResponse(workflow_id='00000000-0000-0000-0000-000000000000', case_id='00000000-0000-0000-0000-000000000000', status='domain_analysis', started_at='2024-01-01T00:00:00'); print(r.domain_results_summary)"` -- field exists and defaults to None.
3. Review that the pipeline stages are: Triage -> Orchestrator -> Parallel Domain -> Strategy -> HITL -> Status Update -> Processing Complete.
  </verify>
  <done>
- AnalysisStatusResponse extended with domain_results_summary field
- Status endpoint detects domain_analysis stage from domain agent execution records
- Status correctly transitions: pending -> triage -> orchestrating -> domain_analysis -> complete
- Backward compatible: orchestrator-complete without domain agents still shows "complete"
- Pipeline docstrings updated to reflect full 8-step pipeline
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 6 domain agent pipeline:
- 4 domain agents (Financial, Legal, Evidence, Strategy) with Pydantic schemas, prompts, execution functions
- Parallel execution via asyncio.gather with independent DB sessions
- Pro-to-Flash fallback with SSE warning events
- HITL confirmation for low-confidence findings
- Full pipeline integration (Triage -> Orchestrator -> Domain -> Strategy -> HITL -> Complete)
  </what-built>
  <how-to-verify>
1. Start the backend: `cd backend && python -m uvicorn app.main:app --reload`
2. Upload 1-2 evidence files to a case via the frontend Case Library
3. Start analysis by clicking the analysis button (or `curl -X POST http://localhost:8000/api/cases/{case_id}/analyze -H "Authorization: Bearer {token}"`)
4. Open the Command Center page for the case
5. Verify in the Command Center:
   - Triage node lights up (processing -> complete)
   - Orchestrator node lights up (processing -> complete)
   - Domain agent nodes appear and light up (financial, legal, evidence -- whichever are routed)
   - Strategy node appears after domain agents complete
   - Thinking traces stream in real-time for each agent
   - If any agent falls back to Flash, a warning appears in the thinking trace
6. Check backend logs for:
   - "Pipeline starting stage=domain_agents" log line
   - "Pipeline starting stage=strategy" log line
   - Domain agent completion logs with duration and token counts
   - Any HITL confirmation triggers (if findings have low confidence)
7. Check analysis status endpoint: `curl http://localhost:8000/api/cases/{case_id}/analysis/{workflow_id}` -- should show domain_results_summary
8. Verify pipeline still works for triage-only case (orchestrator failure): check that files still get marked ANALYZED
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
- Pipeline runs end-to-end: Triage -> Orchestrator -> Domain Agents -> Strategy -> Complete
- SSE events emitted for all agent lifecycle stages
- Low-confidence findings trigger HITL confirmation modal
- Fallback events appear when Pro model fails
- Pipeline status endpoint shows correct stage
- Partial failures in domain agents don't crash pipeline
- File statuses correctly transition to ANALYZED after all agents complete
- Existing triage-only flow still works (backward compatible)
</verification>

<success_criteria>
- Upload files, start analysis, and see all 6 agent types (triage, orchestrator, financial, legal, evidence, strategy) execute in the Command Center
- Each agent shows in the decision tree with thinking traces, token counts, and timing
- Strategy agent runs AFTER parallel agents and incorporates their summaries
- Any low-confidence findings pause the pipeline for user confirmation
- Pipeline completes successfully even if one domain agent fails
- Analysis status endpoint returns domain_results_summary
- Pipeline still works when orchestrator_output is None (triage-only flow preserved)
</success_criteria>

<output>
After completion, create `.planning/phases/06-domain-agents/06-05-SUMMARY.md`
</output>
