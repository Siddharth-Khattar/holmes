---
phase: 07-knowledge-storage-and-domain-agent-enrichment
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - backend/app/services/kg_builder.py
  - backend/app/services/findings_service.py
  - backend/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "KG Builder reads domain agent output_data from agent_executions and extracts ALL entities without filtering or discarding"
    - "KG Builder creates relationship edges between entities co-occurring in the same finding"
    - "Entity deduplication auto-merges exact matches (name_normalized + entity_type) and flags fuzzy matches (>85% similarity) for later LLM resolution"
    - "Merged entities use soft merge via merged_into_id (no hard deletes)"
    - "Findings service saves each domain agent finding to case_findings with citations and entity_ids"
    - "Full-text search on case_findings works via plainto_tsquery"
    - "Degree computation updates entity connection counts"
  artifacts:
    - path: "backend/app/services/kg_builder.py"
      provides: "Programmatic KG Builder: entity extraction, relationship inference, deduplication, degree computation"
      contains: "async def build_knowledge_graph"
    - path: "backend/app/services/findings_service.py"
      provides: "case_findings storage and full-text search"
      contains: "async def save_findings_from_output"
  key_links:
    - from: "backend/app/services/kg_builder.py"
      to: "backend/app/models/knowledge_graph.py"
      via: "ORM model creation"
      pattern: "KgEntity\\("
    - from: "backend/app/services/kg_builder.py"
      to: "backend/app/schemas/agent.py"
      via: "Reading DomainEntity from output"
      pattern: "DomainEntity"
    - from: "backend/app/services/findings_service.py"
      to: "backend/app/models/findings.py"
      via: "ORM model creation"
      pattern: "CaseFinding\\("
---

<objective>
Implement the programmatic KG Builder service and findings storage service.

Purpose: The KG Builder is the core programmatic (non-LLM) service that transforms domain agent structured output into knowledge graph entities and relationships. The findings service persists each finding for downstream search and display. Together they bridge the gap between agent output and queryable knowledge.

Output: 2 new service files, 1 dependency added (rapidfuzz).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-RESEARCH.md
@.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-CONTEXT.md
@backend/app/schemas/agent.py
@backend/app/models/knowledge_graph.py
@backend/app/models/findings.py
@backend/app/agents/domain_agent_runner.py
@backend/app/services/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install rapidfuzz and implement KG Builder service</name>
  <files>
    backend/pyproject.toml
    backend/app/services/kg_builder.py
  </files>
  <action>
    First, install rapidfuzz: `cd backend && uv add rapidfuzz`

    Then create `backend/app/services/kg_builder.py` with 2-line ABOUTME comment.

    Follow the service pattern from `file_service.py` -- standalone async functions taking DB session as parameter, explicit logging.

    **Core functions:**

    1. `normalize_entity_name(name: str) -> str` -- Strips whitespace, lowercases, removes punctuation via `str.translate(str.maketrans("", "", string.punctuation))`. Used for deduplication matching.

    2. `async def extract_entities_from_output(output_data: dict, agent_type: str, execution_id: UUID, case_id: UUID, db: AsyncSession) -> list[KgEntity]` -- The main entity extraction function.
       - Parse output_data using the appropriate Pydantic model (FinancialOutput, LegalOutput, EvidenceOutput, StrategyOutput) based on agent_type. Use a dict mapping: `{"financial": FinancialOutput, "legal": LegalOutput, "evidence": EvidenceOutput, "strategy": StrategyOutput}`. Wrap in try/except for backward compat -- if parsing fails (old records), log warning and return empty list.
       - Extract ALL entities from:
         a. Top-level `output.entities` list
         b. Per-finding `finding.entities` lists (iterate output.findings)
       - For EACH entity, create a KgEntity row: name=entity.value, name_normalized=normalize_entity_name(entity.value), entity_type=entity.type, domain=agent_type, confidence=entity.confidence, metadata={m.key: m.value for m in entity.metadata} if entity.metadata else None, context=entity.context, source_execution_id=execution_id, source_finding_index=idx.
       - db.add() each entity, then await db.flush() to get IDs.
       - NEVER filter or discard entities. Log count.
       - Return the list of created KgEntity objects.

    3. `async def build_relationships_from_findings(output_data: dict, agent_type: str, execution_id: UUID, case_id: UUID, entity_map: dict[str, UUID], db: AsyncSession) -> list[KgRelationship]` -- Infer relationships from co-occurrence.
       - entity_map: maps normalized_name to entity_id (built from extract_entities_from_output results).
       - For each finding in output.findings, collect all entity names referenced.
       - For every pair of entities within the same finding, create a KgRelationship if one doesn't already exist for this pair+case.
       - relationship_type: infer from domain (e.g., "co_mentioned_in_{category}"). Label: "{entity_a} - {entity_b} ({finding.category})".
       - strength: start at 20 per co-occurrence. If entities co-occur in multiple findings, increase by 20 each time (capped at 100). Use a local dict to track cumulative strength before writing.
       - db.add() each relationship, flush.
       - Return created relationships.

    4. `async def deduplicate_entities(case_id: UUID, db: AsyncSession) -> tuple[int, int]` -- Entity deduplication pass.
       - Query all KgEntity rows for case_id where merged_into_id IS NULL.
       - Group by (entity_type) -- cross-domain merging allowed per CONTEXT.md decision. Do NOT group by domain.
       - For exact matches (same name_normalized within same entity_type): pick the entity with lowest created_at as primary. Set merged_into_id on duplicates. Increment merge_count on primary. Update kg_relationships pointing to duplicates to point to primary.
       - For fuzzy matches: use `rapidfuzz.fuzz.ratio(a.name_normalized, b.name_normalized)`. If >= 85, record as fuzzy match in a list (do NOT auto-merge). Log these for Phase 8 LLM resolution.
       - Return tuple of (exact_merges_count, fuzzy_flags_count).

    5. `async def compute_entity_degrees(case_id: UUID, db: AsyncSession) -> None` -- Update degree column.
       - For each entity where merged_into_id IS NULL, count relationships where entity is source or target.
       - Update degree column. Use a single SQL query with subquery or window function for efficiency:
         ```python
         from sqlalchemy import func, case as sa_case, or_
         # Query relationship counts per entity
         # Update degrees in bulk
         ```

    6. `async def build_knowledge_graph(case_id: str, workflow_id: str, domain_results: dict[str, list[tuple[BaseModel | None, str]]], db: AsyncSession) -> tuple[int, int, int]` -- Top-level orchestrator.
       - For each agent_type and its results in domain_results:
         - Query the AgentExecution record to get execution_id and output_data.
         - Call extract_entities_from_output.
         - Build entity_map from results.
         - Call build_relationships_from_findings.
       - After all agents processed, call deduplicate_entities.
       - Call compute_entity_degrees.
       - Return (entities_created, relationships_created, exact_merges).

    **Important design notes:**
    - The function receives domain_results from pipeline.py but ALSO needs to query agent_executions for the output_data JSONB (domain_results contains BaseModel objects, but the KG Builder should read from DB for reliability).
    - Actually, since domain_results already contains the parsed BaseModel objects, use those directly. Only fall back to DB query if domain_results entry is None.
    - Use `from __future__ import annotations` for forward references.
    - Import Pydantic models lazily inside functions to avoid circular imports (follow pipeline.py pattern).
  </action>
  <verify>
    Run `cd backend && python -c "from app.services.kg_builder import build_knowledge_graph, normalize_entity_name, deduplicate_entities; print('KG Builder importable')"` to confirm no import errors.

    Run `cd backend && python -c "from app.services.kg_builder import normalize_entity_name; assert normalize_entity_name('  John Q. Public  ') == 'john q public'; print('Normalization works')"` to test the normalizer.

    Run `cd backend && python -c "import rapidfuzz; print(f'rapidfuzz {rapidfuzz.__version__} installed')"` to confirm dependency.
  </verify>
  <done>KG Builder service with entity extraction, relationship inference, deduplication, and degree computation. rapidfuzz installed.</done>
</task>

<task type="auto">
  <name>Task 2: Implement findings storage service with full-text search</name>
  <files>
    backend/app/services/findings_service.py
  </files>
  <action>
    Create `backend/app/services/findings_service.py` with 2-line ABOUTME comment.

    **Core functions:**

    1. `async def save_findings_from_output(output_data: dict, agent_type: str, execution_id: UUID, case_id: UUID, workflow_id: UUID, file_group_label: str, db: AsyncSession) -> list[CaseFinding]` -- Saves findings from a domain agent output.
       - Parse output_data using appropriate Pydantic model (same mapping as KG Builder).
       - For each finding in output.findings:
         - Create CaseFinding row: case_id, workflow_id, agent_type, agent_execution_id=execution_id, file_group_label, category=finding.category, title=finding.title, finding_text=finding.description (use finding.description since findings_text may not be populated yet -- but if output has findings_text field and it's not None, append it to the description for richer searchable text), confidence=finding.confidence, citations=[c.model_dump(mode="json") for c in finding.citations] (JSONB), entity_ids=[] (populated later after KG entity extraction).
         - db.add() the finding.
       - await db.flush().
       - Return created findings.

    2. `async def update_finding_entity_ids(finding_id: UUID, entity_ids: list[str], db: AsyncSession) -> None` -- Update entity_ids JSONB after KG entities are created.

    3. `async def search_findings(db: AsyncSession, case_id: UUID, query: str, limit: int = 20) -> list[tuple[CaseFinding, float]]` -- Full-text search using tsvector.
       - Use SQLAlchemy's `func.plainto_tsquery('english', query)` and `func.ts_rank()`.
       - Query case_findings where case_id matches AND search_vector @@ tsquery.
       - Order by ts_rank descending, limit results.
       - Return list of (finding, relevance_score) tuples.
       - Use raw SQL text for the @@ operator since SQLAlchemy's support for tsvector operators is limited:
         ```python
         from sqlalchemy import text as sa_text, column, literal_column
         stmt = (
             select(
                 CaseFinding,
                 func.ts_rank(
                     literal_column("search_vector"),
                     func.plainto_tsquery("english", query),
                 ).label("rank"),
             )
             .where(
                 CaseFinding.case_id == case_id,
                 literal_column("search_vector").op("@@")(
                     func.plainto_tsquery("english", query)
                 ),
             )
             .order_by(literal_column("rank").desc())
             .limit(limit)
         )
         ```

    4. `async def list_findings(db: AsyncSession, case_id: UUID, agent_type: str | None = None, category: str | None = None, limit: int = 50, offset: int = 0) -> tuple[list[CaseFinding], int]` -- Paginated listing with optional filters.
       - Build query with optional where clauses for agent_type and category.
       - Return (findings_list, total_count).

    5. `async def get_finding_by_id(db: AsyncSession, finding_id: UUID, case_id: UUID) -> CaseFinding | None` -- Single finding lookup.
  </action>
  <verify>
    Run `cd backend && python -c "from app.services.findings_service import save_findings_from_output, search_findings, list_findings; print('Findings service importable')"` to confirm no import errors.
  </verify>
  <done>Findings service with save, search (tsvector), list, and get operations. All functions take db session as parameter.</done>
</task>

</tasks>

<verification>
1. KG Builder extracts entities from all 4 domain agent output types
2. Relationships are inferred from co-occurrence within findings
3. Deduplication: exact match auto-merges, fuzzy flags at 85%+
4. Merged entities use soft merge (merged_into_id, not deletion)
5. Degree computation counts connections correctly
6. Findings service saves all findings with citations
7. Full-text search returns ranked results
8. rapidfuzz dependency added to pyproject.toml
</verification>

<success_criteria>
- KG Builder processes domain_results dict and creates KgEntity + KgRelationship rows
- Entity deduplication merges exact matches, flags fuzzy matches
- Findings service persists findings with citations as JSONB
- Full-text search works on case_findings via tsvector
- No entities or relationships are lost/filtered during extraction
</success_criteria>

<output>
After completion, create `.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-03-SUMMARY.md`
</output>
