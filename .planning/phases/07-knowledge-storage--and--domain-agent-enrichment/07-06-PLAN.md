---
phase: 07-knowledge-storage-and-domain-agent-enrichment
plan: 06
type: execute
wave: 3
depends_on: ["07-03", "07-05"]
files_modified:
  - backend/app/services/agent_events.py
  - backend/app/services/pipeline.py
autonomous: true

must_haves:
  truths:
    - "SSE events fire when findings are committed and KG entities are added during pipeline"
    - "Pipeline wiring calls findings service after domain agents complete"
    - "Pipeline wiring calls KG Builder after findings are saved"
    - "Pipeline backfills finding-to-entity links after KG build"
    - "Strategy result is included in KG building by adding it to domain_results before calling build_knowledge_graph"
    - "Full pipeline flow: Triage -> Orchestrator -> Domain -> Strategy -> HITL -> Save Findings -> Build KG -> Backfill Entity IDs -> Final"
  artifacts:
    - path: "backend/app/services/agent_events.py"
      provides: "New SSE event types: FINDING_COMMITTED, KG_ENTITY_ADDED, KG_RELATIONSHIP_ADDED"
      contains: "FINDING_COMMITTED"
    - path: "backend/app/services/pipeline.py"
      provides: "Updated pipeline with KG Builder and findings storage stages"
      contains: "build_knowledge_graph"
  key_links:
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/kg_builder.py"
      via: "Pipeline calls KG Builder after domain agents"
      pattern: "build_knowledge_graph"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/findings_service.py"
      via: "Pipeline saves findings and backfills entity_ids"
      pattern: "save_findings_from_output"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/findings_service.py"
      via: "Pipeline backfills entity_ids after KG build"
      pattern: "update_finding_entity_ids"
---

<objective>
Add SSE events for knowledge operations and wire KG Builder + findings storage into the analysis pipeline.

Purpose: This plan connects the services built in Plans 03/05 to the live pipeline. After domain agents complete, the pipeline now automatically saves findings, builds the knowledge graph, backfills finding-to-entity links, and emits SSE events so the frontend can update in real-time.

Output: Updated SSE event types, updated pipeline with 3 new stages.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-RESEARCH.md
@.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-CONTEXT.md
@backend/app/services/pipeline.py
@backend/app/services/agent_events.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SSE event types and emitter functions</name>
  <files>
    backend/app/services/agent_events.py
  </files>
  <action>
    **backend/app/services/agent_events.py changes:**

    1. Add 3 new event types to `AgentEventType` enum:
       ```python
       FINDING_COMMITTED = "finding-committed"
       KG_ENTITY_ADDED = "kg-entity-added"
       KG_RELATIONSHIP_ADDED = "kg-relationship-added"
       ```

    2. Add 3 convenience emitter functions following the existing pattern (e.g., `emit_agent_started`):

       ```python
       async def emit_finding_committed(
           case_id: str,
           finding_id: str,
           agent_type: str,
           title: str,
       ) -> None:
           await publish_agent_event(
               case_id,
               AgentEventType.FINDING_COMMITTED,
               {
                   "type": AgentEventType.FINDING_COMMITTED.value,
                   "findingId": finding_id,
                   "agentType": agent_type,
                   "title": title,
               },
           )

       async def emit_kg_entity_added(
           case_id: str,
           entity_id: str,
           entity_name: str,
           entity_type: str,
       ) -> None:
           await publish_agent_event(
               case_id,
               AgentEventType.KG_ENTITY_ADDED,
               {
                   "type": AgentEventType.KG_ENTITY_ADDED.value,
                   "entityId": entity_id,
                   "entityName": entity_name,
                   "entityType": entity_type,
               },
           )

       async def emit_kg_relationship_added(
           case_id: str,
           relationship_id: str,
           source_entity_name: str,
           target_entity_name: str,
           relationship_type: str,
       ) -> None:
           await publish_agent_event(
               case_id,
               AgentEventType.KG_RELATIONSHIP_ADDED,
               {
                   "type": AgentEventType.KG_RELATIONSHIP_ADDED.value,
                   "relationshipId": relationship_id,
                   "sourceEntityName": source_entity_name,
                   "targetEntityName": target_entity_name,
                   "relationshipType": relationship_type,
               },
           )
       ```
  </action>
  <verify>
    Run `cd backend && python -c "from app.services.agent_events import AgentEventType; assert hasattr(AgentEventType, 'FINDING_COMMITTED'); assert hasattr(AgentEventType, 'KG_ENTITY_ADDED'); print('New SSE event types registered')"` to confirm new event types.

    Run `cd backend && python -c "from app.services.agent_events import emit_finding_committed, emit_kg_entity_added, emit_kg_relationship_added; print('New SSE emitters importable')"` to confirm emitters.
  </verify>
  <done>3 new SSE event types + emitter functions added to agent_events.py following existing patterns.</done>
</task>

<task type="auto">
  <name>Task 2: Wire KG Builder and findings storage into pipeline.py</name>
  <files>
    backend/app/services/pipeline.py
  </files>
  <action>
    **CRITICAL: Read pipeline.py first to understand the current code structure.**

    The `domain_results` variable in pipeline.py has this exact type and shape:
    ```python
    domain_results: dict[str, list[tuple[BaseModel | None, str]]]
    # Key: agent_type string (e.g., "financial", "legal", "evidence")
    # Value: list of (parsed_output_or_None, group_label) tuples
    # Example: {"financial": [(FinancialOutput(...), "group_1"), (None, "group_2")]}
    ```
    This is defined around line 389-391 and populated by `run_domain_agents_parallel()` around line 563.

    The `strategy_result` variable is a separate `StrategyOutput | None` defined around line 663.

    Insert KG Builder and findings storage AFTER the HITL stage (Stage 5, after the last HITL confirmation block, around line ~918) and BEFORE the "Final: Update file statuses to ANALYZED" section (currently around line ~920).

    **Step 1: Add imports.**

    Add to the existing `from app.services.agent_events import ...` statement at the top of the file:
    ```python
    emit_finding_committed,
    emit_kg_entity_added,
    emit_kg_relationship_added,
    ```

    Add lazy imports INSIDE `run_analysis_workflow` (alongside existing lazy imports around lines 145-159):
    ```python
    from app.services.kg_builder import build_knowledge_graph
    from app.services.findings_service import save_findings_from_output, update_finding_entity_ids
    ```

    **Step 2: Add Stage 6 -- Save Findings.**

    Insert after HITL block:

    ```python
    # ---- Stage 6: Save Findings to case_findings ----
    logger.info(
        "Pipeline starting stage=save_findings case=%s workflow=%s",
        case_id, workflow_id,
    )
    all_saved_findings: list = []
    for domain_agent, domain_run_list in domain_results.items():
        for domain_output, grp_label in domain_run_list:
            if domain_output is None:
                continue
            # Query execution record for this agent+group
            exec_result = await db.execute(
                select(AgentExecution)
                .where(
                    AgentExecution.workflow_id == UUID(workflow_id),
                    AgentExecution.agent_name == domain_agent,
                )
                .order_by(AgentExecution.created_at.desc())
                .limit(1)
            )
            agent_exec = exec_result.scalar_one_or_none()
            execution_id = agent_exec.id if agent_exec else None

            saved = await save_findings_from_output(
                output=domain_output,
                agent_type=domain_agent,
                execution_id=execution_id,
                case_id=UUID(case_id),
                workflow_id=UUID(workflow_id),
                file_group_label=grp_label,
                db=db,
            )
            all_saved_findings.extend(saved)
            for f in saved:
                await emit_finding_committed(
                    case_id=case_id,
                    finding_id=str(f.id),
                    agent_type=domain_agent,
                    title=f.title,
                )

    # Also save strategy findings if available
    if strategy_result:
        strat_exec_result2 = await db.execute(
            select(AgentExecution)
            .where(
                AgentExecution.workflow_id == UUID(workflow_id),
                AgentExecution.agent_name == "strategy",
            )
            .order_by(AgentExecution.created_at.desc())
            .limit(1)
        )
        strat_exec2 = strat_exec_result2.scalar_one_or_none()
        strat_saved = await save_findings_from_output(
            output=strategy_result,
            agent_type="strategy",
            execution_id=strat_exec2.id if strat_exec2 else None,
            case_id=UUID(case_id),
            workflow_id=UUID(workflow_id),
            file_group_label="strategy",
            db=db,
        )
        all_saved_findings.extend(strat_saved)
        for f in strat_saved:
            await emit_finding_committed(
                case_id=case_id,
                finding_id=str(f.id),
                agent_type="strategy",
                title=f.title,
            )

    await db.commit()  # Commit findings before KG building
    ```

    **Step 3: Add Stage 7 -- Build Knowledge Graph.**

    ```python
    # ---- Stage 7: Build Knowledge Graph ----
    logger.info(
        "Pipeline starting stage=kg_builder case=%s workflow=%s",
        case_id, workflow_id,
    )
    # Add strategy to domain_results so KG Builder processes strategy entities too
    if strategy_result:
        domain_results.setdefault("strategy", []).append((strategy_result, "strategy"))

    entities_created, relationships_created, exact_merges = await build_knowledge_graph(
        case_id=case_id,
        workflow_id=workflow_id,
        domain_results=domain_results,
        db=db,
    )
    await db.commit()  # Commit KG data

    logger.info(
        "KG build complete case=%s entities=%d relationships=%d merges=%d",
        case_id, entities_created, relationships_created, exact_merges,
    )
    ```

    NOTE: `build_knowledge_graph` is called WITHOUT a `strategy_result` parameter. Instead, strategy_result is added to domain_results dict BEFORE the call. This matches the build_knowledge_graph signature defined in Plan 03: `build_knowledge_graph(case_id, workflow_id, domain_results, db)`.

    **Step 4: Add Stage 7b -- Backfill Finding Entity IDs.**

    After KG build completes, backfill entity_ids on each saved finding. This links findings to their KG entities.

    ```python
    # ---- Stage 7b: Backfill finding-to-entity links ----
    # For each saved finding, find KG entities that came from the same agent execution
    # and link them via the entity_ids JSONB field.
    for finding in all_saved_findings:
        if finding.agent_execution_id is None:
            continue
        # Query entities created from this execution
        entity_result = await db.execute(
            select(KgEntity.id)
            .where(
                KgEntity.source_execution_id == finding.agent_execution_id,
                KgEntity.merged_into_id.is_(None),
            )
        )
        linked_entity_ids = [str(eid) for (eid,) in entity_result.all()]
        if linked_entity_ids:
            await update_finding_entity_ids(
                finding_id=finding.id,
                entity_ids=linked_entity_ids,
                db=db,
            )
    await db.commit()
    ```

    Note: Import `KgEntity` as a lazy import alongside the other lazy imports added in Step 1:
    ```python
    from app.models.knowledge_graph import KgEntity
    ```

    **Step 5: Update processing-complete event.**

    Update the `emit_processing_complete` call to include actual KG entity and relationship counts if those variables are in scope. The existing code counts domain entities from output objects -- add the KG counts:
    ```python
    entities_created=total_entities + total_domain_entities + entities_created,
    relationships_created=relationships_created,
    ```
    Adjust variable names to match the surrounding code's naming conventions.

    Run `make generate-types && make format` after all changes.
  </action>
  <verify>
    Run `cd backend && python -c "from app.services.pipeline import run_analysis_workflow; print('Pipeline importable')"` to confirm no circular import or syntax errors.

    Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes && make generate-types && make format` to confirm type generation and formatting pass.
  </verify>
  <done>Pipeline updated with 3 new stages: Save Findings (Stage 6), Build KG (Stage 7), Backfill Entity IDs (Stage 7b). SSE events fire for each committed finding. Strategy included in KG via domain_results injection. Full pipeline: Triage -> Orchestrator -> Domain -> Strategy -> HITL -> Save Findings -> Build KG -> Backfill Entity IDs -> Final.</done>
</task>

</tasks>

<verification>
1. SSE event types FINDING_COMMITTED, KG_ENTITY_ADDED, KG_RELATIONSHIP_ADDED exist
2. Pipeline calls save_findings_from_output for each domain agent result
3. Pipeline calls save_findings_from_output for strategy result
4. Pipeline adds strategy_result to domain_results BEFORE calling build_knowledge_graph
5. Pipeline calls build_knowledge_graph WITHOUT strategy_result parameter
6. Pipeline backfills entity_ids on saved findings after KG build
7. processing-complete event includes real entity/relationship counts
8. Type check passes, format applied
</verification>

<success_criteria>
- 3 new SSE event types registered
- Pipeline wiring: findings saved, KG built, entity_ids backfilled, events emitted
- Strategy processed via domain_results injection (not separate parameter)
- Finding-to-entity linking happens via update_finding_entity_ids backfill
- Full pipeline: Triage -> Orchestrator -> Domain -> Strategy -> HITL -> Findings -> KG -> Backfill -> Final
- `make generate-types && make format` succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/07-knowledge-storage--and--domain-agent-enrichment/07-06-SUMMARY.md`
</output>
