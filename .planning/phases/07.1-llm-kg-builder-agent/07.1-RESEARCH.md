# Phase 7.1: LLM-Based KG Builder Agent - Research

**Researched:** 2026-02-08
**Domain:** Google ADK LLM Agent, Gemini Pro structured output, Knowledge Graph construction
**Confidence:** HIGH

## Summary

This phase replaces the existing programmatic KG Builder (`backend/app/services/kg_builder.py`) with an LLM-based agent (Gemini Pro) that reads ALL domain agent outputs holistically and produces a curated knowledge graph. The codebase already has well-established patterns for ADK agent execution, Pro-to-Flash fallback, stage-isolated sessions, SSE event publishing, and structured JSON output parsing -- all of which the KG Builder agent should follow.

The primary implementation challenge is designing the Pydantic output schema for the LLM, assembling the input prompt from heterogeneous sources (case_findings rich text + DomainEntity structured lists + case description), and handling the DB schema evolution (new columns on existing tables). The existing `DomainAgentRunner` template method pattern (used by Strategy) provides the execution skeleton, but the KG Builder is structurally different: it receives text-only input (no multimodal files), runs after ALL domain agents complete, and writes to KG tables rather than producing findings.

**Primary recommendation:** Create a new `KgBuilderAgentRunner` class (similar to `StrategyAgentRunner` pattern) that subclasses `DomainAgentRunner` for the execution shell but overrides content preparation to assemble findings text + entities + case context. Use the existing `extract_structured_json` parsing pipeline. Evolve the KG DB models with an Alembic migration for new columns (aliases, description_brief, description_detailed, domains, source_finding_ids on entities; evidence_excerpt, source_finding_ids, temporal_context, corroboration_count on relationships). Wire into pipeline.py as a replacement for the current `build_knowledge_graph()` call.

## Standard Stack

### Core (Already in Project)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| google-adk | Current | ADK LlmAgent, Runner, Session, Events | Project's agent execution framework |
| google-genai | Current | Gemini API types (Content, Part, ThinkingConfig) | Low-level Gemini primitives |
| pydantic | v2 | Output schema validation | All agent outputs use Pydantic models |
| sqlalchemy | 2.x async | Database ORM for KG tables | Project's DB layer |
| alembic | Current | Schema migrations | Project's migration tool |

### Supporting (Already in Project)
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| rapidfuzz | Current | Fuzzy entity matching | Currently used in kg_builder.py dedup -- NO LONGER NEEDED with LLM dedup |
| asyncio | stdlib | Concurrent execution | Pipeline orchestration |

### New Dependencies
None required. The LLM-based KG Builder uses only libraries already in the project.

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Single LLM call | Separate entity + relationship extraction passes | Two passes cost 2x tokens but allow larger contexts; chunk-and-merge handles this instead |
| ADK LlmAgent | Raw google-genai API | ADK provides session management, callbacks, thinking planner -- consistent with all other agents |

## Architecture Patterns

### Recommended Project Structure
```
backend/app/
├── agents/
│   ├── kg_builder.py              # KgBuilderAgentRunner class + run_kg_builder() function
│   └── prompts/
│       └── kg_builder.py          # KG_BUILDER_SYSTEM_PROMPT constant
├── services/
│   ├── kg_builder.py              # REPLACE: old programmatic builder -> new LLM builder orchestrator
│   └── kg_writer.py               # NEW: DB write logic (clear + bulk insert entities/relationships)
├── schemas/
│   └── kg_builder.py              # NEW: KgBuilderOutput, KgBuilderEntity, KgBuilderRelationship Pydantic models
├── models/
│   └── knowledge_graph.py         # EVOLVE: add new columns (aliases, descriptions, domains, etc.)
└── services/
    └── pipeline.py                # MODIFY: replace build_knowledge_graph() call with run_kg_builder()
```

### Pattern 1: Agent Runner Subclass (Follow Strategy Pattern)
**What:** Create `KgBuilderAgentRunner` subclassing `DomainAgentRunner` with custom `_prepare_content()` and `_get_output_type()`.
**When to use:** For the KG Builder agent execution.
**Why:** The Strategy agent (`backend/app/agents/strategy.py`) already demonstrates this exact pattern -- subclassing `DomainAgentRunner` to override content preparation for agents that receive text-only input (no multimodal files).

```python
# Source: backend/app/agents/strategy.py (existing pattern)
class KgBuilderAgentRunner(DomainAgentRunner[KgBuilderOutput]):
    """KG Builder agent runner with findings-aware content preparation."""

    def get_agent_name(self) -> str:
        return "kg_builder"

    def _get_output_type(self) -> type[KgBuilderOutput]:
        return KgBuilderOutput

    def _create_agent_instance(
        self,
        case_id: str,
        model: str,
        publish_fn: PublishFn | None,
    ) -> LlmAgent:
        return AgentFactory.create_kg_builder_agent(
            case_id, model=model, publish_fn=publish_fn
        )

    async def _prepare_content(
        self,
        files: list[CaseFile],  # unused -- KG Builder has no files
        gcs_bucket: str,
        hypotheses: list[dict[str, object]],
        context_injection: str | None = None,
        **kwargs: object,
    ) -> types.Content:
        findings_text = str(kwargs.get("findings_text", ""))
        entities_json = str(kwargs.get("entities_json", ""))
        case_description = str(kwargs.get("case_description", ""))

        prompt_parts: list[str] = []
        if case_description:
            prompt_parts.append(f"--- CASE DESCRIPTION ---\n{case_description}\n---\n")
        prompt_parts.append("Build a knowledge graph from the following domain agent outputs.\n")
        if findings_text:
            prompt_parts.append(f"\n--- DOMAIN AGENT FINDINGS (RICH TEXT) ---\n{findings_text}\n---\n")
        if entities_json:
            prompt_parts.append(f"\n--- DOMAIN AGENT ENTITIES (STRUCTURED) ---\n{entities_json}\n---\n")

        return types.Content(
            role="user",
            parts=[types.Part(text="\n".join(prompt_parts))],
        )
```

### Pattern 2: Factory Method for Agent Creation
**What:** Add `AgentFactory.create_kg_builder_agent()` static method following existing pattern.
**When to use:** Called by the runner to create fresh LlmAgent instances.

```python
# Source: backend/app/agents/factory.py (existing pattern)
@staticmethod
def create_kg_builder_agent(
    case_id: str,
    *,
    model: str = MODEL_PRO,
    publish_fn: PublishFn | None = None,
) -> LlmAgent:
    from app.agents.prompts.kg_builder import KG_BUILDER_SYSTEM_PROMPT
    from app.schemas.kg_builder import KgBuilderOutput

    callbacks = create_agent_callbacks(case_id, publish_fn) if publish_fn else None
    return _create_llm_agent(
        name=_safe_name("kgbuilder", case_id),
        model=model,
        instruction=KG_BUILDER_SYSTEM_PROMPT,
        planner=create_thinking_planner("high"),
        output_schema=KgBuilderOutput,
        output_key="kg_builder_result",
        callbacks=callbacks,
    )
```

### Pattern 3: Clear-and-Rebuild DB Write
**What:** Delete all existing KG data for the case, then bulk-insert fresh curated entities and relationships.
**When to use:** Every time the KG Builder runs (per CONTEXT.md "clean slate every run").

```python
# Approach: Delete old, insert new, compute degrees
async def write_kg_from_llm_output(
    case_id: UUID,
    output: KgBuilderOutput,
    execution_id: UUID | None,
    db: AsyncSession,
) -> tuple[int, int]:
    # 1. Delete old KG data
    await db.execute(delete(KgRelationship).where(KgRelationship.case_id == case_id))
    await db.execute(delete(KgEntity).where(KgEntity.case_id == case_id))
    await db.flush()

    # 2. Insert entities (assign UUIDs, build name -> UUID map)
    entity_map: dict[str, UUID] = {}
    for entity in output.entities:
        kg_entity = KgEntity(...)
        db.add(kg_entity)
        await db.flush()
        entity_map[entity.name] = kg_entity.id

    # 3. Insert relationships (resolve source/target via entity_map)
    for rel in output.relationships:
        source_id = entity_map.get(rel.source_entity)
        target_id = entity_map.get(rel.target_entity)
        if source_id and target_id:
            db.add(KgRelationship(...))

    # 4. Compute degrees
    await compute_entity_degrees(case_id, db)
    await db.flush()
```

### Pattern 4: Input Assembly from Multiple Sources
**What:** Gather all findings text + all DomainEntity lists + case description into a single prompt.
**When to use:** Building the KG Builder's input content.

The input comes from three sources:
1. **case_findings table** -- rich markdown `finding_text` with citations (from `save_findings_from_output`)
2. **agent_executions.output_data** -- raw DomainEntity lists (JSON from domain agent structured output)
3. **cases.description** -- case description for context

```python
# Assembly approach:
async def assemble_kg_builder_input(
    case_id: UUID,
    domain_results: dict[str, list[DomainRunResult]],
    db: AsyncSession,
) -> tuple[str, str, str]:
    """Assemble findings text, entities JSON, and case description."""
    # 1. Query all case_findings for this case (already saved in Stage 6)
    findings = await db.execute(
        select(CaseFinding)
        .where(CaseFinding.case_id == case_id)
        .order_by(CaseFinding.agent_type, CaseFinding.created_at)
    )
    all_findings = list(findings.scalars().all())

    findings_text = _format_findings_for_kg_builder(all_findings)

    # 2. Extract DomainEntity lists from domain_results
    entities_json = _format_entities_for_kg_builder(domain_results)

    # 3. Get case description
    case = await db.execute(select(Case).where(Case.id == case_id))
    case_obj = case.scalar_one_or_none()
    case_description = case_obj.description or "" if case_obj else ""

    return findings_text, entities_json, case_description
```

### Pattern 5: Pipeline Integration Point
**What:** Replace the current `build_knowledge_graph()` call in pipeline.py Stage 7.
**When to use:** After Stage 6 (save_findings) completes.

Current pipeline.py lines 968-995 call `build_knowledge_graph()`. This is the exact insertion point. The new code will:
1. Emit KG_BUILDER_STARTED SSE event
2. Assemble input from findings + entities + case description
3. Call `run_kg_builder()` with Pro-to-Flash fallback
4. Write output to KG tables via `write_kg_from_llm_output()`
5. Emit KG_BUILDER_COMPLETE SSE event
6. If KG Builder fails, log error + emit failure event + continue pipeline (no blocking)

### Anti-Patterns to Avoid
- **Sending multimodal files to KG Builder:** The KG Builder receives TEXT only (findings + entity lists). No GCS files. This keeps the context window focused on analysis, not raw documents.
- **Reusing programmatic dedup logic:** The LLM handles deduplication holistically. Do NOT run `deduplicate_entities()` after the LLM output -- the LLM IS the deduplicator.
- **Using `DomainAgentConfig` instead of subclassing:** The KG Builder's `_prepare_content` is too different from standard domain agents (no files, no context_injection). Use the Strategy-style subclass pattern.
- **Blocking pipeline on KG Builder failure:** Per CONTEXT.md, KG Builder failure must NOT block the pipeline. Wrap in try/except, log, emit error SSE, continue.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Agent execution with retries + fallback | Custom retry/fallback loop | `DomainAgentRunner.run()` template method | Already handles PENDING->RUNNING->COMPLETED, Pro-to-Flash, execution records, token tracking |
| Session management | Manual session creation | `get_or_create_stage_session()` + `create_stage_runner()` | Stage-isolated sessions are already implemented |
| JSON extraction from LLM response | Custom text parsing | `extract_structured_json()` from `parsing.py` | Handles markdown fences, truncated JSON, Pydantic validation |
| SSE event publishing | Custom pub/sub | `emit_agent_started()`, `emit_agent_complete()`, `emit_agent_error()`, `publish_agent_event()` | Full SSE infrastructure exists |
| Agent callbacks for thinking traces | Manual event wiring | `create_agent_callbacks()` + `create_sse_publish_fn()` | Wires all 6 ADK callbacks |
| Entity degree computation | Custom SQL | `compute_entity_degrees()` from current kg_builder.py | Already correct, reuse after LLM writes |

**Key insight:** The codebase has extremely well-factored reusable infrastructure. The DomainAgentRunner, factory pattern, parsing helpers, SSE emitters, and session management handle ~80% of what the KG Builder agent needs. The custom work is: output schema, prompt engineering, input assembly, and DB write logic.

## Common Pitfalls

### Pitfall 1: Gemini Structured Output Schema Limitations
**What goes wrong:** Gemini's constrained decoding rejects schemas with `additionalProperties`, discriminated unions, or deeply nested optional types.
**Why it happens:** The project already encountered this -- `MetadataEntry` was created specifically because `dict` types generate `additionalProperties` in JSON schema.
**How to avoid:** Use flat Pydantic models with explicit fields. Use `list[MetadataEntry]` instead of `dict`. Avoid `Union` types. Use `str` with `Field(description=...)` for free-form fields instead of Literal unions. Test the output schema with a real Gemini call before committing.
**Warning signs:** `400 Bad Request` from Gemini API mentioning schema validation.

### Pitfall 2: Context Window Overflow
**What goes wrong:** Large cases with many findings + entities exceed Gemini Pro's 1M token context window.
**Why it happens:** Rich markdown findings text can be verbose. 10+ domain agent runs with 20+ findings each = substantial text.
**How to avoid:** Implement token estimation before sending. If estimated tokens exceed a threshold (e.g., 800K), use the chunk-and-merge strategy from CONTEXT.md. Chunk findings into groups, run KG Builder on each chunk, then run a merge pass.
**Warning signs:** Gemini API returns 400 or 429 with "token limit exceeded" errors.

### Pitfall 3: Entity Name Matching Between LLM Output and DB
**What goes wrong:** The LLM outputs entity names in its response, and relationships reference those names. If the relationship references a name that doesn't match any entity (typo, different casing), the relationship is orphaned.
**Why it happens:** LLMs are not perfectly consistent with entity names across different parts of their output.
**How to avoid:** Use normalized name matching (lowercase, strip punctuation) when resolving relationship source/target to entity UUIDs. The existing `normalize_entity_name()` function can be reused. Also consider having the LLM output entities with integer IDs and relationships referencing those IDs.
**Warning signs:** Relationships with unresolved source/target entities logged during DB write.

### Pitfall 4: KG Model Schema Migration
**What goes wrong:** New columns (aliases, description_brief, description_detailed, domains JSONB, source_finding_ids) need an Alembic migration. Forgetting this causes runtime errors.
**Why it happens:** The existing KgEntity model has `domain` (singular String) and `context` (Text). The new design needs `domains` (JSONB list), `aliases` (JSONB list), `description_brief` (String), `description_detailed` (Text).
**How to avoid:** Create the Alembic migration FIRST, then update the SQLAlchemy model, then update the API response schemas. Test the migration on a fresh database.
**Warning signs:** SQLAlchemy `ProgrammingError` about missing columns.

### Pitfall 5: ADK Single-Parent Violation
**What goes wrong:** Reusing an LlmAgent instance that already has a parent raises `ValueError` in ADK.
**Why it happens:** ADK enforces single-parent for agent objects. The factory pattern exists to prevent this.
**How to avoid:** Always create fresh agent instances via `AgentFactory.create_kg_builder_agent()`. Never cache or reuse agent objects.
**Warning signs:** `ValueError: agent already has a parent` from ADK.

### Pitfall 6: Partial LLM Output / Lenient Parsing
**What goes wrong:** The LLM produces a mostly-valid output but some entities or relationships are malformed.
**Why it happens:** Large structured outputs are harder for LLMs to maintain schema compliance throughout. Token limits can truncate output mid-JSON.
**How to avoid:** Parse the top-level KgBuilderOutput first. Then iterate entities and relationships, wrapping each in try/except. Log and skip malformed items. Return partial results. This matches the CONTEXT.md "lenient validation" decision.
**Warning signs:** Pydantic `ValidationError` on individual items but valid top-level structure.

### Pitfall 7: Pipeline Stage Ordering
**What goes wrong:** KG Builder runs before findings are saved, so it can't query case_findings.
**Why it happens:** The pipeline saves findings in Stage 6 and builds KG in Stage 7. The new LLM KG Builder needs the saved findings.
**How to avoid:** Keep the same stage ordering. The KG Builder replaces Stage 7 (lines 968-995 in pipeline.py). By this point, Stage 6 has already saved all findings and committed.
**Warning signs:** Empty findings query results inside KG Builder input assembly.

### Pitfall 8: Execution Record for Non-Domain Agents
**What goes wrong:** The `DomainAgentRunner.run()` creates an execution record with `agent_name`, `input_data`, etc. If the KG Builder uses `files=[]` (no files), the execution record's `file_ids` and `file_names` will be empty.
**Why it happens:** `DomainAgentRunner.run()` builds `input_data` from the files list.
**How to avoid:** Override or customize the input_data in the run method, or accept empty file fields. The execution record is for audit -- empty file lists are fine for a text-only agent. Can enrich input_data via kwargs to include `finding_count`, `entity_count`, etc.
**Warning signs:** Confusing execution records with 0 files.

## Code Examples

### KG Builder Output Schema (Pydantic)
```python
# Source: Derived from CONTEXT.md decisions + existing schema patterns

class KgBuilderEntity(BaseModel):
    """A curated entity for the knowledge graph."""
    name: str = Field(..., description="Primary canonical name for this entity")
    entity_type: str = Field(
        ...,
        description="One of: PERSON, ORGANIZATION, LOCATION, EVENT, ASSET, "
        "FINANCIAL_ENTITY, COMMUNICATION, DOCUMENT, OTHER",
    )
    aliases: list[str] = Field(
        default_factory=list,
        description="Alternative names, abbreviations, or references to this entity",
    )
    description_brief: str = Field(
        ..., max_length=200,
        description="One-liner summary for graph tooltips",
    )
    description_detailed: str = Field(
        ..., max_length=2000,
        description="2-4 sentence paragraph synthesized from all findings mentioning this entity",
    )
    domains: list[str] = Field(
        ...,
        description="All domains this entity appears in (e.g., ['financial', 'legal'])",
    )
    confidence: float = Field(
        ..., ge=0, le=100,
        description="How confident we are this entity exists and is correctly identified",
    )
    source_finding_ids: list[str] = Field(
        default_factory=list,
        description="IDs of case_findings that mention this entity",
    )
    properties: list[MetadataEntry] = Field(
        default_factory=list,
        description="Additional metadata (timestamps, amounts, etc.)",
    )
    other_type_explanation: str | None = Field(
        default=None,
        description="Required when entity_type is OTHER -- explain why no core type fits",
    )


class KgBuilderRelationship(BaseModel):
    """A semantic relationship between two entities."""
    source_entity: str = Field(
        ..., description="Name of the source entity (must match an entity name above)",
    )
    target_entity: str = Field(
        ..., description="Name of the target entity (must match an entity name above)",
    )
    relationship_type: str = Field(
        ..., description="Free-form label (e.g., 'employed_by', 'transferred_funds_to', 'co-signed lease')",
    )
    label: str = Field(
        ..., max_length=200,
        description="Human-readable edge label for graph display",
    )
    evidence_excerpt: str = Field(
        ..., max_length=500,
        description="Exact quote from source material supporting this relationship",
    )
    source_finding_ids: list[str] = Field(
        default_factory=list,
        description="IDs of case_findings that evidence this relationship",
    )
    temporal_context: str | None = Field(
        default=None,
        description="When this relationship existed/occurred (e.g., '2023-Q3', 'January 2024')",
    )
    confidence: float = Field(
        ..., ge=0, le=100,
        description="Confidence in this relationship's accuracy",
    )
    strength: int = Field(
        default=50, ge=0, le=100,
        description="Edge weight for graph visualization",
    )


class KgBuilderOutput(BaseModel):
    """Complete knowledge graph output from the LLM KG Builder agent."""
    entities: list[KgBuilderEntity] = Field(
        ..., description="Curated, deduplicated list of investigation-relevant entities",
    )
    relationships: list[KgBuilderRelationship] = Field(
        ..., description="Semantic relationships between entities with evidence citations",
    )
```

### DB Write Logic with Lenient Parsing
```python
# Pattern: parse valid, skip malformed
async def write_kg_from_llm_output(
    case_id: UUID,
    output: KgBuilderOutput,
    execution_id: UUID | None,
    db: AsyncSession,
) -> tuple[int, int]:
    entities_written = 0
    relationships_written = 0
    entity_name_to_id: dict[str, UUID] = {}

    for entity_data in output.entities:
        try:
            kg_entity = KgEntity(
                case_id=case_id,
                name=entity_data.name,
                name_normalized=normalize_entity_name(entity_data.name),
                entity_type=entity_data.entity_type,
                domain=",".join(entity_data.domains),  # or use JSONB if migrated
                confidence=entity_data.confidence,
                context=entity_data.description_detailed,
                properties={"aliases": entity_data.aliases, ...},
                source_execution_id=execution_id,
            )
            db.add(kg_entity)
            await db.flush()
            entity_name_to_id[normalize_entity_name(entity_data.name)] = kg_entity.id
            entities_written += 1
        except Exception as exc:
            logger.warning("Skipping malformed entity %s: %s", entity_data.name, exc)

    for rel_data in output.relationships:
        try:
            source_id = entity_name_to_id.get(normalize_entity_name(rel_data.source_entity))
            target_id = entity_name_to_id.get(normalize_entity_name(rel_data.target_entity))
            if not source_id or not target_id:
                logger.warning(
                    "Skipping relationship %s -> %s: unresolved entity",
                    rel_data.source_entity, rel_data.target_entity,
                )
                continue
            kg_rel = KgRelationship(
                case_id=case_id,
                source_entity_id=source_id,
                target_entity_id=target_id,
                relationship_type=rel_data.relationship_type,
                label=rel_data.label,
                strength=rel_data.strength,
                source_execution_id=execution_id,
                properties={
                    "evidence_excerpt": rel_data.evidence_excerpt,
                    "temporal_context": rel_data.temporal_context,
                    "source_finding_ids": rel_data.source_finding_ids,
                    "confidence": rel_data.confidence,
                },
            )
            db.add(kg_rel)
            relationships_written += 1
        except Exception as exc:
            logger.warning("Skipping malformed relationship: %s", exc)

    await db.flush()
    return entities_written, relationships_written
```

### Pipeline Integration
```python
# In pipeline.py, replacing lines 968-995 (Stage 7: Build Knowledge Graph)

# ---- Stage 7: LLM KG Builder (replaces programmatic builder) ----
logger.info(
    "Pipeline starting stage=kg_builder case=%s workflow=%s",
    case_id, workflow_id,
)

kg_builder_task_id = str(uuid4())
await emit_agent_started(
    case_id=case_id,
    agent_type="kg_builder",
    task_id=kg_builder_task_id,
    file_id="",
    file_name="knowledge-graph-builder",
)

try:
    from app.agents.kg_builder import run_kg_builder

    kg_entities_created, kg_relationships_created = await run_kg_builder(
        case_id=case_id,
        workflow_id=workflow_id,
        user_id=user_id,
        domain_results=domain_results,
        db_session=db,
        publish_event=publish_fn,
    )

    await emit_agent_complete(
        case_id=case_id,
        agent_type="kg_builder",
        task_id=kg_builder_task_id,
        result={...},
    )
except Exception as exc:
    logger.exception("KG Builder failed for case=%s: %s", case_id, exc)
    kg_entities_created = 0
    kg_relationships_created = 0
    await emit_agent_error(
        case_id=case_id,
        agent_type="kg_builder",
        task_id=kg_builder_task_id,
        error=str(exc)[:500],
    )
# Pipeline continues regardless of KG Builder success/failure
```

### SSE Event Pattern
```python
# Three-tier SSE for KG Builder per CONTEXT.md:
# 1. STARTED (agent-started event type)
await emit_agent_started(case_id, "kg_builder", task_id, "", "knowledge-graph-builder")

# 2. Progress updates (thinking-update event type)
await emit_thinking_update(case_id, "kg_builder", "Analyzing domain findings...")
await emit_thinking_update(case_id, "kg_builder", "Extracting entities and relationships...")
await emit_thinking_update(case_id, "kg_builder", "Writing curated knowledge graph to database...")

# 3. COMPLETE (agent-complete event type)
await emit_agent_complete(case_id, "kg_builder", task_id, result_dict)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Programmatic co-occurrence KG Builder | LLM-based holistic KG Builder | Phase 7.1 (this phase) | Semantic relationships instead of co-occurrence; cross-domain inference; natural dedup by LLM |
| Per-domain entity extraction with fuzzy dedup | Single LLM pass seeing ALL findings together | Phase 7.1 (this phase) | Better deduplication -- LLM resolves coreference across domains |
| `domain` as singular String field | `domains` as multi-domain list (JSONB) | Phase 7.1 (this phase) | Entities can span multiple domains |

**Deprecated/outdated after this phase:**
- `kg_builder.py` functions: `extract_entities_from_output()`, `build_relationships_from_findings()`, `deduplicate_entities()` -- all replaced by LLM agent
- `normalize_entity_name()` -- KEEP for DB write (still useful for name->UUID resolution)
- `compute_entity_degrees()` -- KEEP (still needed post-LLM-write)
- `rapidfuzz` dependency for entity dedup -- no longer needed by KG builder (may still be used elsewhere)

## DB Schema Evolution

### KgEntity New/Modified Columns
| Column | Type | Purpose | Migration Action |
|--------|------|---------|------------------|
| `aliases` | JSONB | List of alternative names/references | ADD (nullable, default null) |
| `description_brief` | String(200) | One-liner for tooltips | ADD (nullable) |
| `description_detailed` | Text | 2-4 sentence synthesis | ADD (nullable) -- replaces `context` semantically |
| `domains` | JSONB | List of source domains (multi-domain) | ADD (nullable) -- `domain` (singular) kept for backward compat |
| `source_finding_ids` | JSONB | Finding IDs linking entity to evidence | ADD (nullable) |

### KgRelationship New/Modified Columns
| Column | Type | Purpose | Migration Action |
|--------|------|---------|------------------|
| `evidence_excerpt` | Text | Exact quote supporting the relationship | ADD (nullable) |
| `source_finding_ids` | JSONB | Finding IDs as evidence chain | ADD (nullable) |
| `temporal_context` | String(200) | When the relationship existed | ADD (nullable) |
| `corroboration_count` | Integer | How many agents found this relationship | ADD (default=1) |
| `confidence` | Float | LLM-assessed relationship confidence | ADD (default=0.0) |

### Migration Strategy
- Single Alembic migration adding all new columns as nullable with defaults
- Existing `domain` column on KgEntity kept for backward compatibility with API
- New `domains` JSONB column contains the authoritative multi-domain list
- `context` column semantically replaced by `description_detailed` but kept for backward compat
- API response schemas updated to expose new fields

## Existing Code to Reuse

| Component | File | How to Reuse |
|-----------|------|-------------|
| `DomainAgentRunner` template method | `agents/domain_agent_runner.py` | Subclass for execution skeleton |
| `StrategyAgentRunner` pattern | `agents/strategy.py` | Follow exact same subclass pattern |
| `AgentFactory._create_llm_agent()` | `agents/factory.py` | Use for agent instance creation |
| `create_thinking_planner("high")` | `agents/base.py` | Use for KG Builder agent config |
| `extract_structured_json()` | `agents/parsing.py` | Use for parsing LLM output |
| `get_or_create_stage_session()` | `services/adk_service.py` | Use for stage-isolated session |
| `create_stage_runner()` | `services/adk_service.py` | Use for Runner creation |
| `emit_agent_started/complete/error()` | `services/agent_events.py` | Use for SSE lifecycle events |
| `emit_thinking_update()` | `services/agent_events.py` | Use for progress updates |
| `normalize_entity_name()` | `services/kg_builder.py` | Reuse for name->UUID resolution |
| `compute_entity_degrees()` | `services/kg_builder.py` | Reuse for post-write degree computation |
| `build_strategy_context()` | `agents/domain_runner.py` | Reference pattern for assembling text from domain results |
| `DomainRunResult` dataclass | `agents/domain_runner.py` | Already used in pipeline for passing results |
| `save_findings_from_output()` | `services/findings_service.py` | Already called before KG Builder in pipeline |

## Existing Code to Replace

| Component | File | What Changes |
|-----------|------|-------------|
| `build_knowledge_graph()` | `services/kg_builder.py` | Top-level orchestrator replaced by LLM agent call |
| `extract_entities_from_output()` | `services/kg_builder.py` | LLM extracts entities from text, not Pydantic objects |
| `build_relationships_from_findings()` | `services/kg_builder.py` | LLM infers semantic relationships, not co-occurrence |
| `deduplicate_entities()` | `services/kg_builder.py` | LLM deduplicates holistically |
| Pipeline Stage 7 (lines 968-1003) | `services/pipeline.py` | Replace `build_knowledge_graph()` call with `run_kg_builder()` |

## Open Questions

1. **Entity ID Referencing in LLM Output**
   - What we know: Relationships reference entities by name. Names can be inconsistent.
   - What's unclear: Should we ask the LLM to output entities with integer IDs and have relationships reference those IDs instead of names?
   - Recommendation: Use integer IDs (e.g., entity_id: 1, 2, 3) in the LLM output schema. This eliminates name matching issues. The prompt instructs the LLM to assign sequential IDs. Names are still present for human readability.

2. **Chunk-and-Merge Threshold**
   - What we know: Gemini Pro has 1M token context. CONTEXT.md says "chunk and merge if overflow."
   - What's unclear: What token threshold triggers chunking? How to estimate tokens?
   - Recommendation: Use 700K estimated tokens as threshold (leaving headroom). Estimate ~4 chars per token for English text. For the merge pass, send only entity/relationship summaries (not full findings). Implement chunking as a separate code path invoked when estimation exceeds threshold.

3. **Findings Text vs findings_text Field**
   - What we know: Domain agents produce both `findings` (structured) and `findings_text` (rich markdown). Currently `findings_text` is saved but may be null for older outputs.
   - What's unclear: Should KG Builder prefer `findings_text` or reconstruct from structured `findings`?
   - Recommendation: Use `findings_text` when available (it's richer), fall back to reconstructing from `CaseFinding.finding_text` entries in case_findings table. The case_findings table is the canonical source since it's already saved by Stage 6.

4. **Finding ID Resolution**
   - What we know: KG Builder output includes `source_finding_ids`. The LLM needs to know finding IDs to reference them.
   - What's unclear: How to provide finding IDs to the LLM in its input prompt.
   - Recommendation: When assembling input, prefix each finding with its UUID from case_findings table. Format: `[FINDING:uuid] Title - Description...`. The LLM can then reference these IDs in its output.

## Sources

### Primary (HIGH confidence)
- `backend/app/agents/domain_agent_runner.py` -- DomainAgentRunner template method pattern (read line-by-line)
- `backend/app/agents/strategy.py` -- StrategyAgentRunner subclass pattern (read line-by-line)
- `backend/app/agents/factory.py` -- AgentFactory static method pattern (read line-by-line)
- `backend/app/agents/base.py` -- create_thinking_planner, callbacks, PublishFn (read line-by-line)
- `backend/app/agents/parsing.py` -- extract_structured_json generic parser (read line-by-line)
- `backend/app/services/pipeline.py` -- Pipeline Stage 7 insertion point (read line-by-line)
- `backend/app/services/kg_builder.py` -- Current programmatic builder to be replaced (read line-by-line)
- `backend/app/services/agent_events.py` -- SSE event infrastructure (read line-by-line)
- `backend/app/services/adk_service.py` -- Session and runner management (read line-by-line)
- `backend/app/models/knowledge_graph.py` -- KgEntity, KgRelationship ORM models (read line-by-line)
- `backend/app/schemas/knowledge_graph.py` -- API response schemas (read line-by-line)
- `backend/app/schemas/agent.py` -- DomainEntity, DomainAgentOutput, Finding schemas (read line-by-line)
- `backend/app/services/findings_service.py` -- CaseFinding persistence (read line-by-line)
- `backend/app/config.py` -- Settings with model names, thresholds (read line-by-line)
- `backend/alembic/versions/c7a1f8d23e51_add_knowledge_tables.py` -- Current KG table schema (read line-by-line)
- `.planning/phases/07.1-llm-kg-builder-agent/07.1-CONTEXT.md` -- User decisions (read line-by-line)

### Secondary (MEDIUM confidence)
- `.claude/projects/.../memory/architecture-decisions.md` -- Architecture overview
- `.claude/projects/.../memory/MEMORY.md` -- Project-wide conventions

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all libraries already in project, patterns well-established
- Architecture: HIGH -- directly follows existing StrategyAgentRunner + DomainAgentRunner patterns
- DB evolution: HIGH -- exact column additions identified from CONTEXT.md decisions vs current schema
- Pitfalls: HIGH -- identified from reading actual codebase (MetadataEntry workaround, single-parent, etc.)
- Output schema: MEDIUM -- Pydantic schema design is informed by CONTEXT.md but not yet tested against Gemini constrained decoding
- Chunk-and-merge: LOW -- threshold and merge strategy are estimates; need runtime validation

**Research date:** 2026-02-08
**Valid until:** 2026-03-08 (stable domain -- ADK patterns unlikely to change in 30 days)
