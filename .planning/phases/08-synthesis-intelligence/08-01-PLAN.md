---
phase: 08-synthesis-intelligence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/models/investigation_task.py
  - backend/app/models/case.py
  - backend/app/models/__init__.py
  - backend/alembic/versions/f8a3b2c91d40_add_investigation_tasks_and_verdict_cols.py
  - backend/app/schemas/synthesis.py
autonomous: true

must_haves:
  truths:
    - "Investigation tasks persist per case and survive restarts"
    - "Case verdict data (label + summary) persists across page refreshes and restarts"
    - "Synthesis Agent output can be parsed into structured typed schemas without data loss"
    - "API responses serialize all synthesis data types with proper UUID handling"
  artifacts:
    - path: "backend/app/models/investigation_task.py"
      provides: "InvestigationTask SQLAlchemy model"
      contains: "investigation_tasks"
    - path: "backend/app/schemas/synthesis.py"
      provides: "SynthesisOutput + all nested Pydantic schemas + API response schemas"
      exports: ["SynthesisOutput", "SynthesisHypothesis", "SynthesisContradiction", "SynthesisGap", "SynthesisTimelineEvent", "SynthesisTask", "SynthesisKeyFinding", "SynthesisVerdict"]
    - path: "backend/alembic/versions/f8a3b2c91d40_add_investigation_tasks_and_verdict_cols.py"
      provides: "Alembic migration for investigation_tasks table + Case verdict columns"
  key_links:
    - from: "backend/app/schemas/synthesis.py"
      to: "backend/app/models/synthesis.py"
      via: "Schema field names match DB model columns for write_synthesis_output"
      pattern: "class Synthesis"
---

<objective>
Create the database schema additions and Pydantic schemas required by the Synthesis Agent.

Purpose: The Synthesis Agent needs an `investigation_tasks` table (new) and `verdict_label`/`verdict_summary` columns on the Case model. It also needs a comprehensive `SynthesisOutput` Pydantic schema for Gemini structured output, and API response schemas for all synthesis data endpoints.

Output: New InvestigationTask model, updated Case model, Alembic migration, SynthesisOutput schema, API response Pydantic schemas.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-synthesis-intelligence/08-CONTEXT.md
@.planning/phases/08-synthesis-intelligence/08-RESEARCH.md
@backend/app/models/synthesis.py
@backend/app/models/case.py
@backend/app/models/base.py
@backend/app/schemas/kg_builder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: InvestigationTask model + Case verdict columns + Alembic migration</name>
  <files>
    backend/app/models/investigation_task.py
    backend/app/models/case.py
    backend/app/models/__init__.py
    backend/alembic/versions/f8a3b2c91d40_add_investigation_tasks_and_verdict_cols.py
  </files>
  <action>
1. Create `backend/app/models/investigation_task.py` with ABOUTME comment:
   - Class `InvestigationTask(Base)` with `__tablename__ = "investigation_tasks"`
   - Columns: `id` (UUID PK, gen_random_uuid), `case_id` (UUID FK to cases.id ON DELETE CASCADE), `workflow_id` (UUID not null), `title` (String(300) not null), `description` (Text not null), `task_type` (String(50) not null -- "resolve_contradiction", "obtain_evidence", "verify_hypothesis", "follow_up_interview", "document_retrieval", "external_research", "cross_reference", "expert_consultation"), `priority` (String(20) not null default "medium" -- "low"/"medium"/"high"/"critical"), `status` (String(20) not null default "pending" -- "pending"/"in_progress"/"completed"/"dismissed"), `source_hypothesis_id` (UUID FK to case_hypotheses.id nullable SET NULL), `source_contradiction_id` (UUID FK to case_contradictions.id nullable SET NULL), `source_gap_id` (UUID FK to case_gaps.id nullable SET NULL), `created_at` (DateTime(timezone=True) server_default now())
   - Index on case_id: `Index("idx_investigation_tasks_case_id", "case_id")`
   - Relationship: `case = relationship("Case")`
   - Follow the exact mapped_column pattern from synthesis.py models

2. Add two nullable columns to `Case` model in `backend/app/models/case.py`:
   - `verdict_label: Mapped[str | None] = mapped_column(String(30), nullable=True, comment="Conclusive/Substantial/Inconclusive")` -- add AFTER the `latest_workflow_id` column
   - `verdict_summary: Mapped[str | None] = mapped_column(Text, nullable=True, comment="One-line case summary from synthesis")` -- add right after verdict_label

3. Add `InvestigationTask` import to `backend/app/models/__init__.py` so Alembic sees it.

4. Create manual Alembic migration `backend/alembic/versions/f8a3b2c91d40_add_investigation_tasks_and_verdict_cols.py`:
   - Revision ID: `f8a3b2c91d40`
   - Down revision: `e4b2c1a37f90` (the latest -- evolve_kg_schema_for_llm_builder)
   - `upgrade()`:
     - `op.create_table("investigation_tasks", ...)` with all columns listed above
     - `op.create_index("idx_investigation_tasks_case_id", "investigation_tasks", ["case_id"])`
     - `op.add_column("cases", sa.Column("verdict_label", sa.String(30), nullable=True))`
     - `op.add_column("cases", sa.Column("verdict_summary", sa.Text(), nullable=True))`
   - `downgrade()`:
     - `op.drop_column("cases", "verdict_summary")`
     - `op.drop_column("cases", "verdict_label")`
     - `op.drop_index("idx_investigation_tasks_case_id")`
     - `op.drop_table("investigation_tasks")`
   - Use `sa.text("gen_random_uuid()")` for UUID server defaults
   - Use PG_UUID(as_uuid=True) for UUID columns
  </action>
  <verify>
    Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && python -c "from app.models.investigation_task import InvestigationTask; from app.models.case import Case; print('verdict_label' in Case.__table__.columns); print(InvestigationTask.__tablename__)"` -- should print True and "investigation_tasks".
  </verify>
  <done>InvestigationTask model has all 12 columns + index, Case model has verdict_label and verdict_summary, Alembic migration file exists with correct up/down</done>
</task>

<task type="auto">
  <name>Task 2: SynthesisOutput Pydantic schema + API response schemas</name>
  <files>
    backend/app/schemas/synthesis.py
  </files>
  <action>
Create `backend/app/schemas/synthesis.py` with ABOUTME comment. This file contains TWO categories of schemas:

**Category A: Gemini Structured Output Schema (SynthesisOutput)**

These are the schemas that define the LLM's structured output. They use simple types compatible with Gemini's structured output format. The SynthesisOutput schema must follow the RESEARCH.md design:

1. `SynthesisEvidence(BaseModel)`: `finding_id: str`, `role: str` (supporting/contradicting/neutral), `excerpt: str`

2. `SynthesisHypothesis(BaseModel)`: `claim: str`, `confidence: float` (0-100), `reasoning: str`, `evidence: list[SynthesisEvidence]`

3. `SynthesisContradiction(BaseModel)`: `claim_a: str`, `claim_b: str`, `source_a_finding_id: str`, `source_a_excerpt: str`, `source_b_finding_id: str`, `source_b_excerpt: str`, `severity: str` (minor/significant/critical), `domain: str`

4. `SynthesisGap(BaseModel)`: `description: str`, `what_is_missing: str`, `why_needed: str`, `priority: str` (low/medium/high/critical), `suggested_actions: str`, `related_entity_ids: list[int]`

5. `SynthesisTimelineEvent(BaseModel)`: `title: str`, `description: str`, `event_date: str` (ISO 8601), `event_end_date: str | None = None`, `event_type: str` (transaction/meeting/filing/communication/etc.), `domain: str` (financial/legal/evidence/strategy), `source_finding_ids: list[str]`, `source_entity_ids: list[int]`

6. `SynthesisTask(BaseModel)`: `title: str`, `description: str`, `task_type: str` (resolve_contradiction/obtain_evidence/verify_hypothesis/follow_up_interview/document_retrieval/external_research/cross_reference/expert_consultation), `priority: str`, `source_hypothesis_index: int | None = None`, `source_contradiction_index: int | None = None`, `source_gap_index: int | None = None`

7. `SynthesisKeyFinding(BaseModel)`: `title: str`, `description: str`, `importance_rank: int`, `source_finding_ids: list[str]`

8. `SynthesisVerdict(BaseModel)`: `verdict: str`, `evidence_strength: str` (Conclusive/Substantial/Inconclusive), `key_strengths: list[str]`, `key_weaknesses: list[str]`

9. `SynthesisOutput(BaseModel)`: `case_summary: str`, `case_verdict: SynthesisVerdict`, `key_findings: list[SynthesisKeyFinding]`, `hypotheses: list[SynthesisHypothesis]`, `contradictions: list[SynthesisContradiction]`, `gaps: list[SynthesisGap]`, `timeline_events: list[SynthesisTimelineEvent]`, `investigation_tasks: list[SynthesisTask]`, `cross_modal_links: list[dict[str, str]]`, `cross_domain_conclusions: str`, `risk_assessment: str`, `has_location_data: bool`

**Category B: API Response Schemas**

These are the schemas returned by the API endpoints. They map from DB models to JSON responses with proper UUID serialization.

10. `HypothesisEvidenceResponse(BaseModel)`: `finding_id: str`, `role: str`, `excerpt: str`

11. `HypothesisResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `claim: str`, `status: str`, `confidence: float`, `evidence: list[HypothesisEvidenceResponse]` (computed from supporting_evidence + contradicting_evidence), `source_agent: str | None`, `reasoning: str | None`, `created_at: datetime`
    - Use a `@model_validator(mode="before")` to merge `supporting_evidence` and `contradicting_evidence` from the DB into a flat `evidence` list with role labels. If the DB has the data already split: items from supporting_evidence get role="supporting", items from contradicting_evidence get role="contradicting".

12. `ContradictionResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `claim_a: str`, `claim_b: str`, `source_a: dict | None`, `source_b: dict | None`, `severity: str`, `domain: str | None`, `resolution_status: str`, `created_at: datetime`

13. `GapResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `description: str`, `what_is_missing: str`, `why_needed: str | None`, `priority: str`, `related_entity_ids: list[str] | None`, `suggested_actions: str | None`, `created_at: datetime`

14. `TaskResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `title: str`, `description: str`, `task_type: str`, `priority: str`, `status: str`, `source_hypothesis_id: str | None`, `source_contradiction_id: str | None`, `source_gap_id: str | None`, `created_at: datetime`

15. `KeyFindingResponse(BaseModel)`: `title: str`, `description: str`, `importance_rank: int`, `source_finding_ids: list[str]`

16. `VerdictResponse(BaseModel)`: `verdict: str`, `evidence_strength: str`, `key_strengths: list[str]`, `key_weaknesses: list[str]`

17. `SynthesisResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `case_summary: str | None`, `case_verdict: VerdictResponse | None` (parsed from JSONB), `cross_modal_links: list | None`, `cross_domain_conclusions: list | None`, `key_findings_summary: str | None`, `risk_assessment: str | None`, `timeline_event_count: int`, `created_at: datetime`

18. `TimelineEventResponse(BaseModel, from_attributes=True)`: `id: str`, `case_id: str`, `workflow_id: str`, `title: str`, `description: str | None`, `event_date: datetime | None`, `event_end_date: datetime | None`, `event_type: str | None`, `layer: str | None`, `source_entity_ids: list[str] | None`, `citations: list | None`, `created_at: datetime`

Use `model_config = ConfigDict(from_attributes=True)` pattern (not the class inheritance shorthand). All UUID fields should be serialized as `str` via validators or `ConfigDict`. Follow the exact pattern from `schemas/knowledge_graph.py` for UUID serialization.
  </action>
  <verify>
    Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && python -c "from app.schemas.synthesis import SynthesisOutput, HypothesisResponse, SynthesisResponse, TaskResponse; print('OK')"` -- should print OK.
  </verify>
  <done>SynthesisOutput schema has all 12 fields matching RESEARCH.md design, API response schemas cover all 7 data types with proper UUID serialization and from_attributes=True, HypothesisResponse merges supporting/contradicting evidence into flat list</done>
</task>

</tasks>

<verification>
- `python -c "from app.models.investigation_task import InvestigationTask"` succeeds
- `python -c "from app.schemas.synthesis import SynthesisOutput"` succeeds
- All SynthesisOutput fields match RESEARCH.md SynthesisOutput schema exactly
- InvestigationTask model has FK constraints to case_hypotheses, case_contradictions, case_gaps
- Case model has verdict_label (String(30)) and verdict_summary (Text)
- Migration file has correct down_revision = e4b2c1a37f90
</verification>

<success_criteria>
- InvestigationTask table schema matches REQUIREMENTS.md REQ-TASK-001
- SynthesisOutput covers all output categories from CONTEXT.md: hypotheses, contradictions, gaps, timeline events, tasks, verdict, key findings, cross-domain conclusions, risk assessment, has_location_data
- API response schemas enable serialization from DB models with proper UUID handling
- No `any` types in Pydantic schemas
</success_criteria>

<output>
After completion, create `.planning/phases/08-synthesis-intelligence/08-01-SUMMARY.md`
</output>
