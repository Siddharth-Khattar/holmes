---
phase: 08-synthesis-intelligence
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - backend/app/agents/synthesis.py
  - backend/app/agents/prompts/synthesis.py
  - backend/app/agents/factory.py
  - backend/app/services/pipeline.py
  - backend/app/services/agent_events.py
autonomous: true

must_haves:
  truths:
    - "Synthesis Agent runs as pipeline Stage 8 after KG Builder + Entity Backfill"
    - "Synthesis Agent reads case_findings + kg_entities + kg_relationships from DB"
    - "Synthesis output is written to case_hypotheses, case_contradictions, case_gaps, case_synthesis, timeline_events, and investigation_tasks tables"
    - "Case verdict_label and verdict_summary are updated on the Case model after synthesis"
    - "SSE events fire for synthesis lifecycle (agent-started, agent-complete, synthesis-data-ready)"
    - "Pipeline emits processing-complete only after synthesis stage completes"
  artifacts:
    - path: "backend/app/agents/synthesis.py"
      provides: "SynthesisAgentRunner + assemble_synthesis_input + write_synthesis_output + run_synthesis"
      contains: "class SynthesisAgentRunner"
    - path: "backend/app/agents/prompts/synthesis.py"
      provides: "SYNTHESIS_SYSTEM_PROMPT"
      contains: "SYNTHESIS_SYSTEM_PROMPT"
    - path: "backend/app/agents/factory.py"
      provides: "AgentFactory.create_synthesis_agent method"
      contains: "create_synthesis_agent"
  key_links:
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/agents/synthesis.py"
      via: "run_synthesis() call in Stage 8"
      pattern: "run_synthesis"
    - from: "backend/app/agents/synthesis.py"
      to: "backend/app/schemas/synthesis.py"
      via: "SynthesisOutput import for structured parsing"
      pattern: "from app.schemas.synthesis import SynthesisOutput"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/agent_events.py"
      via: "emit_synthesis_data_ready after DB commit"
      pattern: "emit_synthesis_data_ready"

# Scope decision: REQ-HYPO-006 (per-hypothesis SSE events)
# v1 uses a single batch "synthesis-data-ready" event after all synthesis data is committed.
# Per-item SSE events (e.g., hypothesis-created, contradiction-detected) are deferred.
# Rationale: The synthesis agent produces all outputs in a single LLM call with structured output.
# There is no incremental data to stream -- all items are written atomically after the call completes.
# The batch event is sufficient for the frontend to invalidate React Query caches and refetch.
# Per-item events would add complexity with no UX benefit since items arrive simultaneously.
---

<objective>
Implement the Synthesis Agent backend: runner, prompt, factory method, input assembly, output writer, and pipeline integration.

Purpose: The Synthesis Agent is the intelligence core of Holmes. It reads all domain findings and the curated knowledge graph, then produces hypotheses, contradictions, gaps, timeline events, investigation tasks, case summary, verdict, key findings, and cross-domain conclusions. This is pipeline Stage 8.

Output: Working Synthesis Agent that runs after KG Builder in the pipeline, writes to all synthesis tables, and emits SSE events.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-synthesis-intelligence/08-CONTEXT.md
@.planning/phases/08-synthesis-intelligence/08-RESEARCH.md
@.planning/phases/08-synthesis-intelligence/08-01-SUMMARY.md
@backend/app/agents/kg_builder.py
@backend/app/agents/domain_agent_runner.py
@backend/app/agents/factory.py
@backend/app/services/pipeline.py
@backend/app/services/agent_events.py
@backend/app/models/synthesis.py
@backend/app/models/investigation_task.py
@backend/app/schemas/synthesis.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Synthesis Agent runner + prompt + factory method</name>
  <files>
    backend/app/agents/synthesis.py
    backend/app/agents/prompts/synthesis.py
    backend/app/agents/factory.py
  </files>
  <action>
**1. Create `backend/app/agents/prompts/synthesis.py`** with ABOUTME comment.

Define `SYNTHESIS_SYSTEM_PROMPT` as a multi-line string. The prompt must instruct Gemini Pro to:

- Role: You are the Synthesis Intelligence Agent for Holmes, a legal investigation platform. You cross-reference all domain agent findings and the curated knowledge graph to produce a comprehensive case analysis.

- Input description: You receive:
  - Case metadata (name, description, type)
  - All domain agent findings grouped by agent_type, each prefixed with [FINDING:uuid]
  - Knowledge graph entities prefixed with [ENTITY:uuid:name] including type, descriptions, aliases, domains
  - Knowledge graph relationships with labels, types, evidence excerpts, temporal context
  - File metadata (names and types of uploaded evidence)

- Output requirements (match SynthesisOutput schema exactly):
  1. `case_summary`: Executive summary of the entire case (2-4 paragraphs). Cover the overall narrative, key actors, timeline, and current state of evidence.
  2. `case_verdict`: Overall assessment with `verdict` (free text assessment), `evidence_strength` (MUST be exactly one of: "Conclusive", "Substantial", "Inconclusive"), `key_strengths` (list of case strengths), `key_weaknesses` (list of weaknesses/vulnerabilities).
  3. `key_findings`: Top 5-10 most impactful discoveries ranked by importance. Each has `title`, `description`, `importance_rank` (1=most important), `source_finding_ids` (reference [FINDING:uuid] IDs from input).
  4. `hypotheses`: Up to 10 investigative hypotheses. Each has `claim`, `confidence` (0-100), `reasoning`, and `evidence` list where each item has `finding_id` (from [FINDING:uuid]), `role` ("supporting"/"contradicting"/"neutral"), and `excerpt` (exact text from the finding).
  5. `contradictions`: Detected contradictions between claims. Each has `claim_a`, `claim_b`, `source_a_finding_id`/`source_a_excerpt`, `source_b_finding_id`/`source_b_excerpt`, `severity` ("minor"/"significant"/"critical"), `domain`. Only include genuine contradictions, not minor phrasing differences.
  6. `gaps`: Missing evidence. Each has `description`, `what_is_missing`, `why_needed`, `priority` ("low"/"medium"/"high"/"critical"), `suggested_actions` (specific actionable step, e.g. "Request bank statements from X for period Y"), `related_entity_ids` (integer IDs from [ENTITY:uuid:name] -- use 0-indexed position in the entity list if needed).
  7. `timeline_events`: Up to 30 chronological events. Each has `title`, `description`, `event_date` (ISO 8601 format YYYY-MM-DDTHH:MM:SSZ -- if only year/month known, use first day), `event_end_date` (for duration events, null otherwise), `event_type` (transaction/meeting/filing/communication/discovery/arrest/legal_action/other), `domain` (financial/legal/evidence/strategy), `source_finding_ids`, `source_entity_ids`.
  8. `investigation_tasks`: Tasks derived from contradictions, gaps, and hypotheses. Each has `title`, `description`, `task_type` (resolve_contradiction/obtain_evidence/verify_hypothesis/follow_up_interview/document_retrieval/external_research/cross_reference/expert_consultation), `priority`, and optional `source_hypothesis_index`/`source_contradiction_index`/`source_gap_index` (0-based index into the respective output lists).
  9. `cross_modal_links`: Temporal correlations across modalities. List of dicts with keys like "description", "modality_a", "modality_b", "temporal_link". Can be empty if no cross-modal evidence.
  10. `cross_domain_conclusions`: Integrated narrative prose combining insights from financial + legal + evidence + strategy domains. Write as unified text, not formulaic domain-by-domain. 2-3 paragraphs.
  11. `risk_assessment`: Risk factors and mitigation suggestions. 1-2 paragraphs.
  12. `has_location_data`: Boolean -- set true if ANY findings reference specific geographic locations that would benefit from geospatial analysis.

- Citation rules: Every hypothesis evidence item, contradiction source, and key finding MUST reference specific [FINDING:uuid] IDs from the input. Do not fabricate finding IDs.
- Quality rules: Severity classifications must be meaningful (critical = fundamentally opposing claims that cannot both be true). Confidence scores should reflect actual evidence weight, not be uniformly high.

**2. Create `backend/app/agents/synthesis.py`** with ABOUTME comment.

Follow the KG Builder pattern exactly (`backend/app/agents/kg_builder.py`):

a) `class SynthesisAgentRunner(DomainAgentRunner[SynthesisOutput])`:
   - `get_agent_name()` -> `"synthesis"`
   - `_get_output_type()` -> `SynthesisOutput`
   - `_create_agent_instance(case_id, model, publish_fn)` -> `AgentFactory.create_synthesis_agent(case_id, model=model, publish_fn=publish_fn)`
   - `_prepare_content(files, gcs_bucket, hypotheses, context_injection, **kwargs)`:
     - Extract `synthesis_input = str(kwargs.get("synthesis_input", ""))`
     - Return `types.Content(role="user", parts=[types.Part(text=synthesis_input)])`

b) `async def assemble_synthesis_input(case_id: str, db: AsyncSession) -> str`:
   - Query `Case` for name, description, type
   - Query `CaseFile` for filenames and content_types
   - Query `CaseFinding` grouped by agent_type, ordered by agent_type + created_at. Format each as `[FINDING:{id}] [{agent_type}] {title}\n{finding_text}`
   - Query `KgEntity` ordered by entity_type. Format each as `[ENTITY:{id}:{name}] Type: {entity_type}, Description: {description_brief}, Aliases: {aliases}, Domains: {domains}`
   - Query `KgRelationship` with joined source/target entity names. Format each as `{source_name} --[{label} ({relationship_type})]-> {target_name} | Evidence: {evidence_excerpt} | Temporal: {temporal_context}`
   - Assemble sections: `--- CASE METADATA ---`, `--- FILES ---`, `--- DOMAIN AGENT FINDINGS ---`, `--- KNOWLEDGE GRAPH ENTITIES ---`, `--- KNOWLEDGE GRAPH RELATIONSHIPS ---`
   - Return the assembled string

c) `async def write_synthesis_output(case_id: str, output: SynthesisOutput, workflow_id: str, db: AsyncSession) -> dict[str, int]`:
   - Delete existing synthesis data for this case (delete from investigation_tasks, timeline_events, case_gaps, case_contradictions, case_hypotheses, case_synthesis WHERE case_id matches -- order matters for FKs)
   - `await db.flush()`
   - Write `CaseSynthesis` row: case_summary, case_verdict (serialize SynthesisVerdict to dict for JSONB), cross_modal_links, cross_domain_conclusions (store as JSONB list with single item), key_findings_summary (serialize key_findings list to JSON string), risk_assessment, timeline_event_count = len(output.timeline_events)
   - Write each `CaseHypothesis`: claim, status (derive from confidence: >60 = "SUPPORTED", <40 = "REFUTED", else "PENDING"), confidence (divide by 100 to store as 0-1 float), supporting_evidence (filter evidence where role=="supporting"), contradicting_evidence (filter evidence where role=="contradicting"), source_agent="synthesis", reasoning
   - Write each `CaseContradiction`: claim_a, claim_b, source_a (dict with finding_id + excerpt), source_b (dict with finding_id + excerpt), severity, domain
   - Write each `CaseGap`: description, what_is_missing, why_needed, priority, suggested_actions, related_entity_ids (convert int IDs to strings)
   - Write each `TimelineEvent`: title, description, event_date (parse ISO string to datetime, handle fuzzy dates gracefully with try/except), event_end_date, event_type, layer=domain (the domain field maps to the layer column), source_entity_ids, citations (build from source_finding_ids)
   - Write each `InvestigationTask`: title, description, task_type, priority, status="pending". For source FK references: use hypothesis/contradiction/gap DB UUIDs from the just-written records (maintain lists of written records and index into them using source_*_index).
   - Update `Case` model: set `verdict_label = output.case_verdict.evidence_strength`, `verdict_summary = first 200 chars of case_summary`
   - `await db.flush()`
   - Return dict with counts: {"hypotheses": N, "contradictions": N, "gaps": N, "timeline_events": N, "tasks": N}
   - Wrap each record write in `async with db.begin_nested()` savepoint (same pattern as KG Builder) to isolate malformed items

d) `async def run_synthesis(case_id, workflow_id, user_id, db_session, publish_event) -> dict[str, int]`:
   - Call `assemble_synthesis_input(case_id, db_session)` to get input text
   - Call `SynthesisAgentRunner().run(case_id=case_id, workflow_id=workflow_id, user_id=user_id, files=[], hypotheses=[], db_session=db_session, publish_event=publish_event, synthesis_input=input_text)`
   - If output is None, log warning and return empty counts
   - Call `write_synthesis_output(case_id, output, workflow_id, db_session)`
   - Return the counts dict

**3. Add `create_synthesis_agent` to `backend/app/agents/factory.py`**:

Add a static method following the `create_kg_builder_agent` pattern:

```python
@staticmethod
def create_synthesis_agent(
    case_id: str, *, model: str = MODEL_PRO,
    publish_fn: PublishFn | None = None,
) -> LlmAgent:
    from app.agents.prompts.synthesis import SYNTHESIS_SYSTEM_PROMPT
    from app.schemas.synthesis import SynthesisOutput
    callbacks = create_agent_callbacks(case_id, publish_fn) if publish_fn else None
    return _create_llm_agent(
        name=_safe_name("synthesis", case_id),
        model=model,
        instruction=SYNTHESIS_SYSTEM_PROMPT,
        planner=create_thinking_planner("high"),
        output_schema=SynthesisOutput,
        output_key="synthesis_result",
        callbacks=callbacks,
    )
```

Also add `"synthesis"` to the `AGENT_TYPE_ALIASES` dict if it exists (check factory.py for this pattern).
  </action>
  <verify>
    Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && python -c "from app.agents.synthesis import SynthesisAgentRunner, assemble_synthesis_input, run_synthesis; from app.agents.factory import AgentFactory; agent = AgentFactory.create_synthesis_agent('test-case-id'); print(agent.name, 'OK')"` -- should print the agent name and OK.
  </verify>
  <done>SynthesisAgentRunner subclass, assemble_synthesis_input reads from 5 tables, write_synthesis_output writes to 6 tables + updates Case, SYNTHESIS_SYSTEM_PROMPT covers all 12 output fields, factory method creates synthesis agent with Pro model and high thinking</done>
</task>

<task type="auto">
  <name>Task 2: Pipeline Stage 8 integration + SSE events</name>
  <files>
    backend/app/services/pipeline.py
    backend/app/services/agent_events.py
  </files>
  <action>
**1. Add `SYNTHESIS_DATA_READY` event type to `backend/app/services/agent_events.py`**:

Add to the `AgentEventType` enum (after FINDING_COMMITTED or at the end):
```python
SYNTHESIS_DATA_READY = "synthesis-data-ready"
```

Add a new emit helper function:
```python
async def emit_synthesis_data_ready(
    case_id: str,
    counts: dict[str, int],
) -> None:
    """Emit synthesis-data-ready event after all synthesis data is committed to DB."""
    await _publish_agent_event(
        case_id=case_id,
        event_type=AgentEventType.SYNTHESIS_DATA_READY,
        data={
            "type": AgentEventType.SYNTHESIS_DATA_READY.value,
            "caseId": case_id,
            "counts": counts,
        },
    )
```

**Scope note on SSE granularity (REQ-HYPO-006):** This plan implements a single batch `synthesis-data-ready` SSE event that fires after ALL synthesis data is committed. Per-item events (e.g., per-hypothesis or per-contradiction) are intentionally omitted for v1. The synthesis agent produces all outputs in a single Gemini structured output call -- there is no incremental data to stream. The batch event triggers React Query cache invalidation on the frontend, which refetches all synthesis data. Per-item events would add SSE complexity with no UX benefit since all items arrive atomically.

**2. Integrate Stage 8 into `backend/app/services/pipeline.py`**:

Find the section after Stage 7b (entity backfill) and BEFORE the "Final: Update file statuses to ANALYZED" section. The exact location is after `await db.commit()` on line ~1064 (after entity backfill commit) and before the `# ---- Final: Update file statuses to ANALYZED ----` comment.

Insert Stage 8 block:

```python
# ---- Stage 8: Synthesis Agent ----
logger.info(
    "Pipeline starting stage=synthesis case=%s workflow=%s",
    case_id,
    workflow_id,
)
synthesis_task_id = str(uuid4())
await emit_agent_started(
    case_id=case_id,
    agent_type="synthesis",
    task_id=synthesis_task_id,
    file_id="",
    file_name="synthesis-agent",
)

synthesis_counts: dict[str, int] = {}
try:
    synthesis_counts = await run_synthesis(
        case_id=case_id,
        workflow_id=workflow_id,
        user_id=user_id,
        db_session=db,
        publish_event=publish_fn,
    )
    await emit_agent_complete(
        case_id=case_id,
        agent_type="synthesis",
        task_id=synthesis_task_id,
        result={
            "taskId": synthesis_task_id,
            "agentType": "synthesis",
            "outputs": [
                {
                    "type": "synthesis-results",
                    "data": synthesis_counts,
                }
            ],
        },
    )
except Exception as exc:
    logger.exception("Synthesis failed for case=%s: %s", case_id, exc)
    await db.rollback()
    await emit_agent_error(
        case_id=case_id,
        agent_type="synthesis",
        task_id=synthesis_task_id,
        error=str(exc)[:500],
    )

await db.commit()  # Commit synthesis data

# Emit synthesis-data-ready AFTER commit so frontend can safely fetch
if synthesis_counts:
    await emit_synthesis_data_ready(
        case_id=case_id,
        counts=synthesis_counts,
    )

logger.info(
    "Synthesis complete case=%s counts=%s",
    case_id,
    synthesis_counts,
)
```

Add the import at the top of pipeline.py:
```python
from app.agents.synthesis import run_synthesis
from app.services.agent_events import emit_synthesis_data_ready
```

Also update the `emit_processing_complete` call near the end to include synthesis counts if available.

**Important**: The `processing-complete` event should now fire AFTER synthesis, not before it. The current code has processing-complete after Stage 7b. Move the processing-complete emission to AFTER Stage 8. This means the file status update to ANALYZED and the processing-complete event should come after synthesis completes.
  </action>
  <verify>
    Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && python -c "from app.services.agent_events import AgentEventType, emit_synthesis_data_ready; print(AgentEventType.SYNTHESIS_DATA_READY.value)"` -- should print "synthesis-data-ready". Then grep pipeline.py for "Stage 8" to verify the block exists.
  </verify>
  <done>Pipeline Stage 8 runs after entity backfill, calls run_synthesis, emits agent-started/agent-complete/synthesis-data-ready SSE events, processing-complete fires after synthesis, synthesis failure is non-blocking (logged + SSE error emitted)</done>
</task>

</tasks>

<verification>
- `python -c "from app.agents.synthesis import run_synthesis"` succeeds
- `python -c "from app.agents.factory import AgentFactory; AgentFactory.create_synthesis_agent('x')"` succeeds
- `grep -n "Stage 8" backend/app/services/pipeline.py` shows synthesis block
- `grep -n "SYNTHESIS_DATA_READY" backend/app/services/agent_events.py` shows new event type
- Pipeline order is: Stage 7 (KG Builder) -> Stage 7b (Entity Backfill) -> Stage 8 (Synthesis) -> File status update -> processing-complete
</verification>

<success_criteria>
- Synthesis Agent follows DomainAgentRunner pattern exactly (subclass, text-only input, structured output)
- Input assembly queries all 5 data sources (case, files, findings, entities, relationships)
- Output writer populates all 6 destination tables + updates Case verdict columns
- Pipeline Stage 8 is positioned correctly and failure is non-blocking
- SSE synthesis-data-ready event fires after DB commit (batch event; per-item events deferred per scope decision)
- SYNTHESIS_SYSTEM_PROMPT covers all 12 SynthesisOutput fields with clear instructions
</success_criteria>

<output>
After completion, create `.planning/phases/08-synthesis-intelligence/08-02-SUMMARY.md`
</output>
