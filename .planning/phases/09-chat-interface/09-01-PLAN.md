---
phase: 09-chat-interface
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/agents/chat_tools.py
  - backend/app/agents/prompts/chat.py
  - backend/app/services/chat_service.py
  - backend/app/api/chat.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "POST /api/cases/:caseId/chat returns SSE stream with chat-token events"
    - "Chat agent queries KG entities and relationships for the case"
    - "Chat agent queries domain findings with filters"
    - "Chat agent retrieves full text of a single finding by ID for deep citation-backed answers"
    - "Chat agent queries synthesis data (hypotheses, contradictions, gaps, timeline, locations)"
    - "Chat agent performs full-text search over findings"
    - "System prompt includes full case synthesis context on every request"
    - "Backend returns 400 when no analysis has been run for the case"
    - "Tool calls emit chat-tool-start and chat-tool-end SSE events"
    - "Response completion emits chat-done SSE event with full message and structured citations"
    - "Chat endpoint enforces auth and case ownership"
  artifacts:
    - path: "backend/app/agents/chat_tools.py"
      provides: "4 closure-based tool factory functions for DB queries"
      exports: ["make_query_knowledge_graph_tool", "make_get_findings_tool", "make_get_synthesis_tool", "make_search_findings_tool"]
    - path: "backend/app/agents/prompts/chat.py"
      provides: "Chat system prompt template with context injection"
      exports: ["build_chat_system_prompt"]
    - path: "backend/app/services/chat_service.py"
      provides: "Chat session management and context loading"
      exports: ["load_chat_context", "create_chat_agent_and_runner"]
    - path: "backend/app/api/chat.py"
      provides: "SSE streaming chat endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/app/api/chat.py"
      to: "backend/app/services/chat_service.py"
      via: "create_chat_agent_and_runner call"
      pattern: "create_chat_agent_and_runner"
    - from: "backend/app/services/chat_service.py"
      to: "backend/app/agents/chat_tools.py"
      via: "tool factory functions"
      pattern: "make_.*_tool"
    - from: "backend/app/api/chat.py"
      to: "sse_starlette.EventSourceResponse"
      via: "SSE streaming"
      pattern: "EventSourceResponse"
    - from: "backend/app/main.py"
      to: "backend/app/api/chat.py"
      via: "router registration"
      pattern: "include_router.*chat"
---

<objective>
Build the complete backend Chat Agent with 4 DB-query tools, system prompt with full case context injection, a chat service layer, and an SSE streaming POST endpoint at `/api/cases/{case_id}/chat`.

Purpose: Enables evidence-backed case Q&A where every answer is grounded in prior analysis data (findings, KG, synthesis, timeline, locations). This is the backend that the existing frontend chatbot will connect to.

Output: 5 new/modified backend files providing a fully functional chat agent endpoint.

Note: Context caching (REQ-AGENT-007f / ContextCacheConfig with 2hr TTL) is DEFERRED from this phase. It is a cost optimization, not a functional requirement. The chat agent works correctly without caching -- it just costs more per query. Context caching requires wrapping the agent in an ADK `App` object with `ContextCacheConfig`, which may need experimentation. It can be added as a follow-up optimization once the core chat is working.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-chat-interface/09-CONTEXT.md
@.planning/phases/09-chat-interface/09-RESEARCH.md

# Key backend files to reference
@backend/app/agents/factory.py
@backend/app/agents/base.py
@backend/app/services/adk_service.py
@backend/app/database.py
@backend/app/api/sse.py
@backend/app/api/synthesis.py
@backend/app/models/findings.py
@backend/app/models/knowledge_graph.py
@backend/app/models/synthesis.py
@backend/app/models/investigation_task.py
@backend/app/services/findings_service.py
@backend/app/main.py
@backend/app/api/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Chat tools and system prompt</name>
  <files>
    backend/app/agents/chat_tools.py
    backend/app/agents/prompts/chat.py
  </files>
  <action>
  Create `backend/app/agents/chat_tools.py` with 4 closure-based tool factory functions. Each factory takes `case_id: str` and returns an async function that ADK will auto-wrap as FunctionTool. Tools use `_get_sessionmaker()` from `app.database` to create independent DB sessions per call (avoids shared session conflicts -- see RESEARCH.md Pitfall 4).

  **Agent naming helper:** Inline the `_safe_name` logic directly in `chat_service.py` (where it's used) rather than importing from `factory.py`. The logic is just 2 lines:
  ```python
  import re
  sanitized = re.sub(r"[^a-zA-Z0-9]", "", case_id[:8])
  name = f"chat_{sanitized}"
  ```
  Do NOT import `_safe_name` from `factory.py`. Keep it self-contained.

  **Tool 1: `make_query_knowledge_graph_tool(case_id)`**
  Returns `async def query_knowledge_graph(entity_type: str | None = None, entity_name_search: str | None = None, limit: int = 50) -> dict`.
  - Query `KgEntity` where `case_id` matches AND `merged_into_id IS NULL`
  - Optional filters: `entity_type` exact match, `entity_name_search` via `ilike`
  - Order by `degree DESC`, limit capped at 100
  - Fetch relationships where source or target entity is in result set, limit 200
  - Return `{"entities": [...], "relationships": [...], "entity_count": N, "relationship_count": N}`
  - Entity fields: name, type (entity_type), description (description_brief), domain, confidence, connections (degree), aliases, source_finding_ids
  - Relationship fields: source (name resolved), target (name resolved), type (relationship_type), label, evidence (evidence_excerpt), temporal_context

  **Tool 2: `make_get_findings_tool(case_id)`**
  Returns `async def get_findings(finding_id: str | None = None, agent_type: str | None = None, category: str | None = None, min_confidence: float = 0.0, limit: int = 50) -> dict`.
  - **Single-finding drill-down mode:** When `finding_id` is provided, fetch that single `CaseFinding` by ID and return its FULL text (not truncated). Ignore all other filter parameters. Return `{"finding": {...}, "mode": "detail"}` with finding fields including complete `finding_text`.
  - **List mode (finding_id is None):** Query `CaseFinding` where `case_id` matches. Optional filters: agent_type, category, confidence >= min_confidence. Order by confidence DESC, limit capped at 100. Return `{"findings": [...], "count": N, "mode": "list"}`. Finding fields: id (str), agent_type, category, title, finding_text (truncated to first 500 chars), confidence, citations (the JSONB array as-is).
  - The docstring MUST document both modes clearly so the LLM knows when to use finding_id for full text retrieval vs. list mode for browsing.

  **Tool 3: `make_get_synthesis_tool(case_id)`**
  Returns `async def get_synthesis(include_hypotheses: bool = True, include_contradictions: bool = True, include_gaps: bool = True, include_timeline: bool = False, include_locations: bool = False, include_tasks: bool = False) -> dict`.
  - Load `CaseSynthesis` for the case (latest by created_at)
  - Conditionally load: CaseHypothesis, CaseContradiction, CaseGap, TimelineEvent, Location, InvestigationTask
  - Return structured dict with counts and data for each included section
  - Hypothesis fields: id (str), claim, status, confidence, supporting_evidence (JSONB), contradicting_evidence (JSONB), reasoning
  - Contradiction fields: id (str), claim_a, claim_b, source_a (JSONB), source_b (JSONB), severity, domain
  - Gap fields: id (str), description, what_is_missing, why_needed, priority, related_entity_ids
  - Timeline event fields: id (str), title, description, event_date (ISO string), event_type, layer
  - Location fields: id (str), name, location_type, coordinates (JSONB)
  - Task fields: id (str), title, task_type, priority, status, description
  - Return synthesis summary fields: case_summary, case_verdict, key_findings_summary, risk_assessment, cross_domain_conclusions

  **Tool 4: `make_search_findings_tool(case_id)`**
  Returns `async def search_findings(query: str, limit: int = 20) -> dict`.
  - Use `func.to_tsvector('english', CaseFinding.finding_text)` and `func.to_tsquery('english', query)` for full-text search
  - Follow the existing pattern in `findings_service.py`
  - Return `{"results": [...], "count": N, "query": query}`
  - Result fields: id (str), agent_type, category, title, finding_text (truncated to 300 chars), confidence, citations

  All tool functions must have comprehensive docstrings (ADK uses these as the tool schema description for the LLM). Use type hints on all parameters (ADK uses these for the schema). Return dicts (not Pydantic models -- ADK serializes dicts as tool responses).

  Start file with:
  ```python
  # ABOUTME: Chat agent tool factories providing 4 DB-query tools for case Q&A.
  # ABOUTME: Each factory captures case_id via closure and creates independent DB sessions per call.
  ```

  Create `backend/app/agents/prompts/chat.py` with a `build_chat_system_prompt(context: dict) -> str` function.

  The function takes a context dict (loaded from DB by chat_service) and builds the system prompt. The prompt must:

  1. Define the agent's role: "You are Holmes, an AI investigative assistant for the case: {case_name}."
  2. Inject full synthesis context: case_summary, case_verdict, key_findings_summary, risk_assessment, cross_domain_conclusions, and counts of all data types (findings, entities, hypotheses, contradictions, gaps, timeline events, locations, tasks).
  3. Define citation format: Instruct the model to cite sources using `[[file_id|locator|label]]` format. Explain each field. Provide 2-3 examples. Emphasize: "Every factual claim must have a citation. If you cannot cite a source, say so explicitly."
  4. Define tool usage strategy: "Try these tools in order: (1) get_synthesis for summary-level questions, (2) query_knowledge_graph for entity/relationship questions, (3) get_findings for domain-specific details (use finding_id parameter to retrieve full text of a specific finding), (4) search_findings for keyword-specific queries."
  5. Define response format: "Respond in markdown. Use headers, bullet lists, bold for emphasis. Keep responses thorough but concise."
  6. Include case context: case name, description, case type, status, verdict label if available.

  Start file with:
  ```python
  # ABOUTME: Chat agent system prompt builder with full case context injection.
  # ABOUTME: Produces investigation-grounded prompts with citation format instructions.
  ```
  </action>
  <verify>
  Run `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && python -m py_compile app/agents/chat_tools.py && python -m py_compile app/agents/prompts/chat.py && echo "OK"`. Then run pyright/type check: `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes && make generate-types` (if applicable) or `cd backend && pyright app/agents/chat_tools.py app/agents/prompts/chat.py`.
  </verify>
  <done>
  4 tool factory functions compile and type-check. get_findings supports single-finding drill-down via finding_id parameter (returns full text) and list mode (returns truncated text). System prompt builder produces a string with case context injected. All tool functions have full docstrings and type hints.
  </done>
</task>

<task type="auto">
  <name>Task 2: Chat service, API endpoint, and router registration</name>
  <files>
    backend/app/services/chat_service.py
    backend/app/api/chat.py
    backend/app/main.py
  </files>
  <action>
  Create `backend/app/services/chat_service.py` with chat session management:

  **`load_chat_context(db: AsyncSession, case_id: UUID) -> dict`**
  - Load `Case` record (name, description, case_type, status, verdict_label, verdict_summary)
  - Load `CaseSynthesis` for the case (latest by created_at) -- extract case_summary, case_verdict (JSONB), key_findings_summary, risk_assessment, cross_domain_conclusions (JSONB)
  - Count: findings, entities, hypotheses, contradictions, gaps, timeline events, locations, tasks
  - Return a dict with all this context data
  - If no synthesis exists (analysis not run), raise an appropriate error (or return a dict with `analysis_available: False`)

  **`create_chat_agent_and_runner(case_id: str, user_id: str, context: dict) -> tuple[Runner, Session]`**
  - Build system prompt via `build_chat_system_prompt(context)`
  - Create 4 tool functions via the tool factories with case_id baked in
  - Inline the agent naming logic (do NOT import `_safe_name` from factory.py):
    ```python
    import re
    sanitized = re.sub(r"[^a-zA-Z0-9]", "", case_id[:8])
    agent_name = f"chat_{sanitized}"
    ```
  - Create LlmAgent directly (NOT via AgentFactory -- chat agent has different config):
    - `name=agent_name`
    - `model=MODEL_FLASH` (Flash for chat speed)
    - `instruction=system_prompt`
    - `tools=[query_knowledge_graph, get_findings, get_synthesis, search_findings]`
    - NO `output_schema` (free-form text, not JSON)
    - NO `planner` (chat doesn't need thinking budget overhead -- speed matters)
    - Do NOT set `temperature` or any `generate_content_config`. Use the default temperature (1.0). Gemini 3 models require temperature=1.0 per ADK limitations -- setting a lower temperature causes errors or unexpected behavior.
  - Create Runner via `create_stage_runner(agent)`
  - **Session management:** Do NOT use `get_or_create_stage_session` -- it requires a `workflow_id` which chat does not have. Instead, use the ADK `session_service` directly:
    1. Import `get_session_service` and `get_settings` from `app.services.adk_service`
    2. Build a deterministic session ID: `hashlib.sha256(f"{case_id}:{user_id}:chat".encode()).hexdigest()` (for persistent sessions), OR use the `session_id` passed from the frontend request (for new-session-on-clear behavior)
    3. Call `session_service.get_session(app_name=settings.adk_app_name, user_id=user_id, session_id=session_id)` to check for existing
    4. If not found, call `session_service.create_session(app_name=settings.adk_app_name, user_id=user_id, session_id=session_id, state={"case_id": case_id})`
  - Return `(runner, session)`

  Start file with:
  ```python
  # ABOUTME: Chat service layer for session management and context loading.
  # ABOUTME: Assembles chat agent with tools and synthesis context for case Q&A.
  ```

  Create `backend/app/api/chat.py` with the SSE streaming POST endpoint:

  **`POST /api/cases/{case_id}/chat`**
  - Request body: `ChatRequest` Pydantic model with `message: str` and `session_id: str | None = None` (None = new session)
  - Auth: `CurrentUser` dependency (same pattern as synthesis.py)
  - Case ownership: Validate user owns the case (same `_get_user_case` pattern or inline the check)
  - Load context via `load_chat_context(db, case_id)` -- if analysis not available, return 400 with error message
  - Create agent and runner via `create_chat_agent_and_runner`
  - Build user message as `types.Content(role="user", parts=[types.Part(text=body.message)])`
  - Stream via `runner.run_async(user_id=user_id, session_id=session_id, new_message=user_message)`
  - Parse ADK events in a generator function:
    - For each event, check `event.content.parts`:
      - `part.text` where NOT `part.thought` -> yield `chat-token` SSE event with `{"text": part.text}`
      - `part.function_call` -> yield `chat-tool-start` SSE event with `{"tool_name": part.function_call.name}`
      - `part.function_response` -> yield `chat-tool-end` SSE event with `{"tool_name": part.function_response.name}`
      - Skip thinking parts (`part.thought == True`)
    - After all events, yield `chat-done` SSE event with `{"message": full_text, "citations": extracted_citations}` where citations are parsed from the full_text using the `[[file_id|locator|label]]` regex
    - On error, yield `chat-error` SSE event with `{"error": str(e)}`
  - Return `EventSourceResponse` with headers `{"X-Accel-Buffering": "no", "Cache-Control": "no-cache"}`
  - Include a heartbeat mechanism if needed (or rely on the SSE library's built-in)

  Citation extraction: Write a helper `_extract_citations(text: str) -> list[dict]` that uses regex `\[\[([a-f0-9-]+)\|([^|]*)\|([^\]]+)\]\]` to find all citation markers and return `[{"file_id": ..., "locator": ..., "label": ...}]`.

  Start file with:
  ```python
  # ABOUTME: SSE streaming chat endpoint for case Q&A with citation-backed responses.
  # ABOUTME: POST /api/cases/{case_id}/chat streams token-by-token via Server-Sent Events.
  ```

  Update `backend/app/main.py`:
  - Import `from app.api import chat`
  - Add `app.include_router(chat.router, tags=["chat"])` after the locations router

  **Important implementation notes:**
  - ADK `runner.run_async()` yields `Event` objects. Each event has `.content` which may be `None`. Check for `None` before accessing `.parts`.
  - Some events are intermediate (tool calls, tool responses). Only stream text parts to the client. Tool call events get separate SSE events.
  - The `event.is_final_response()` method indicates the last event.
  - Handle the case where the LLM calls multiple tools in a single turn (accumulate tool events, stream them individually).
  - Use `uuid.uuid4()` for new session IDs when `session_id` is None.
  </action>
  <verify>
  1. Run type checks: `cd /Users/siddharth/Development/Hackathons/Gemini_3_2025/holmes/backend && source .venv/bin/activate && pyright app/services/chat_service.py app/api/chat.py`
  2. Verify compilation: `python -m py_compile app/services/chat_service.py && python -m py_compile app/api/chat.py && echo "OK"`
  3. Verify main.py imports: `python -c "from app.main import app; print('Router registered')"` (may need environment variables -- if so, just verify the import compiles)
  4. Run `make format` from repo root if available.
  </verify>
  <done>
  Chat API endpoint compiles and type-checks. Router is registered in main.py. `POST /api/cases/{case_id}/chat` returns EventSourceResponse. Context loading queries all synthesis tables. Agent created with Flash model, default temperature (1.0 -- required by Gemini 3), no output_schema, 4 closure-based tools. Session management uses ADK session_service directly (not get_or_create_stage_session). Citation extraction regex works on `[[file_id|locator|label]]` format.
  </done>
</task>

</tasks>

<verification>
1. All 5 files compile without errors (`py_compile`)
2. Type checking passes on all new files (pyright or project typecheck command)
3. Router registered in main.py (chat endpoint accessible at `/api/cases/{case_id}/chat`)
4. Tool functions have complete docstrings and type hints (ADK uses these for schema generation)
5. get_findings tool supports both list mode (truncated text) and single-finding drill-down (full text via finding_id parameter)
6. System prompt includes case context injection and citation format instructions
7. SSE events follow the defined format: chat-token, chat-tool-start, chat-tool-end, chat-done, chat-error
8. No temperature override on the agent (uses default 1.0 required by Gemini 3)
9. Session management uses ADK session_service directly, not get_or_create_stage_session
</verification>

<success_criteria>
- Backend chat agent with 4 DB-query tools compiles and type-checks
- SSE streaming POST endpoint at /api/cases/{case_id}/chat is registered
- System prompt includes full synthesis context and citation format instructions
- Auth and case ownership enforced on the endpoint
- Returns 400 when analysis hasn't been run
- Citation extraction regex parses [[file_id|locator|label]] format
- get_findings supports finding_id parameter for full-text drill-down
- Agent uses default temperature (no override) per Gemini 3 requirements
- Chat session management is independent of pipeline session management (no workflow_id dependency)
</success_criteria>

<output>
After completion, create `.planning/phases/09-chat-interface/09-01-SUMMARY.md`
</output>
